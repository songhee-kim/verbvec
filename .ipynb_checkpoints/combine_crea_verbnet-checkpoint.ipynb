{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, csv\n",
    "from nltk.corpus import verbnet\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn = nltk.corpus.util.LazyCorpusLoader('verbnet3', nltk.corpus.reader.verbnet.VerbnetCorpusReader, r'(?!\\.).*\\.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of verbs in Verbnet\n",
    "#vn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_lemma = vn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_from_outside\n",
    "\n",
    "from collections import Iterable                           \n",
    "\n",
    "def flatten(items):\n",
    "    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element 'FRAME' at 0x1a241da728>,\n",
       " <Element 'FRAME' at 0x1a241dfcc8>,\n",
       " <Element 'FRAME' at 0x1a241e6368>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vn.frames('escape-51.1')\n",
    "#vn.frames('hit-18.1-1')[0]['semantics'][0]\n",
    "\n",
    "#vn.frames('hit-18.1-1')[0]['semantics'][0]\n",
    "\n",
    "#vn.vnclass('hit-18.1-1')\n",
    "\n",
    "\n",
    "vn.vnclass('escape-51.1').findall('FRAMES/FRAME')\n",
    "#vn.vnclass('hit-18.1-1').findall('FRAMES/FRAME')[0]\n",
    "\n",
    "#vn._get_semantics_within_frame(vn.vnclass('escape-51.1').findall('FRAMES/FRAME')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = vn._get_semantics_within_frame(vn.vnclass('hit-18.1-1').findall('FRAMES/FRAME')[0])\n",
    "# pred_list = []\n",
    "# for i in range(len(preds)):\n",
    "#     pred_list.append(preds[i]['predicate_value'])\n",
    "# pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_list = vn.vnclass('hit-18.1-1').findall('FRAMES/FRAME')\n",
    "# pred_list = []\n",
    "# for i in range(len(frame_list)):\n",
    "#     semantics_per_frame = vn._get_semantics_within_frame(frame_list[i])\n",
    "#     for j in range(len(semantics_per_frame)):\n",
    "#         pred = semantics_per_frame[j]['predicate_value']\n",
    "#         pred_list.append(pred)\n",
    "# pred_list = set(pred_list)\n",
    "# print(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of Semantic Predicates given verbnet class (such as 'hit-18.1-1')\n",
    "def semantic_pred_from_class(self, vnclass):\n",
    "    #vnclass is the outcome of self.vnclass(classids)\n",
    "    frame_list = vnclass.findall('FRAMES/FRAME')\n",
    "    pred_list = []\n",
    "    for i in range(len(frame_list)):\n",
    "        semantics_per_frame = self._get_semantics_within_frame(frame_list[i])\n",
    "        for j in range(len(semantics_per_frame)):\n",
    "            pred = semantics_per_frame[j]['predicate_value']\n",
    "            pred_list.append(pred)\n",
    "    pred_list = list(set(pred_list))\n",
    "    \n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['equals', 'path_rel', 'cause', 'transfer', 'cost']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "semantic_pred_from_class(vn, vn.vnclass('obtain-13.5.2-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absorb-39.8',\n",
       " 'accept-77.1',\n",
       " 'accompany-51.7',\n",
       " 'acquiesce-95.1',\n",
       " 'acquiesce-95.1-1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a complete list of classids in VerbNet3.3\n",
    "all_classids = vn.classids()\n",
    "all_classids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "#make a complete list of Semantic Predicates used in VerbNet3.3 (from classids)\n",
    "interim_semantic_preds = []\n",
    "for ci in all_classids: \n",
    "    pred_per_class = semantic_pred_from_class(vn, vn.vnclass(ci))\n",
    "    interim_semantic_preds.append(pred_per_class)\n",
    "all_semantic_preds = sorted(list(set(list(flatten(interim_semantic_preds)))))\n",
    "print(len(all_semantic_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic_pred_from_class(vn, vn.vnclass('put-9.1-2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test of 'flatten' function\n",
    "# l = [['assess'],\n",
    "#  ['transfer',\n",
    "#   'has_possession',\n",
    "#   'financial_interest_in',\n",
    "#   'cause',\n",
    "#   'transfer',\n",
    "#   'has_possession',\n",
    "#   'financial_interest_in',\n",
    "#   'cause'],\n",
    "#  ['motion', 'path_rel', 'path_rel', 'cause']]\n",
    "# l2 = [1,2,3,4,5,6]\n",
    "# list(flatten(l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of Semantic Predicates used for each lemma\n",
    "def semantic_pred_from_lemma(self, lemma):\n",
    "    lemma_classids = self.classids(lemma)\n",
    "    pred_list_combined = []\n",
    "    for k in range(len(lemma_classids)):\n",
    "        vnclass = self.vnclass(lemma_classids[k])\n",
    "        frame_list = vnclass.findall('FRAMES/FRAME')\n",
    "        pred_list = []\n",
    "        for i in range(len(frame_list)):\n",
    "            semantics_per_frame = self._get_semantics_within_frame(frame_list[i])\n",
    "            for j in range(len(semantics_per_frame)):\n",
    "                pred = semantics_per_frame[j]['predicate_value']\n",
    "                pred_list.append(pred)\n",
    "        pred_list_combined.append(pred_list)\n",
    "    \n",
    "    pred_list_combined_final = list(set(list(flatten(pred_list_combined))))\n",
    "\n",
    "\n",
    "    return pred_list_combined_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assess',\n",
       " 'financial_interest_in',\n",
       " 'motion',\n",
       " 'path_rel',\n",
       " 'cause',\n",
       " 'transfer',\n",
       " 'has_possession']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare the results of the following two: \n",
    "semantic_pred_from_lemma(vn, 'put')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### do it later \n",
    "#semantic_pred_from_class(vn, vn.vnclass(vn.classids('put')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst = [1, 2, 3, 4]\n",
    "# lst.append(5) if 5 not in lst else lst\n",
    "\n",
    "# lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbnet.classids()\n",
    "\n",
    "#len(vn.classids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbnet_lemma = verbnet.lemmas()\n",
    "# vn_lemma = vn.lemmas()\n",
    "#this shows that verbnet and vn load different datasets! Stick to vn, not verbnet (IMPORTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbnet_lemma\n",
    "#vn_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x for x in verbnet_lemma if x not in vn_lemma]\n",
    "\n",
    "#[x for x in vn.classids() if x not in verbnet.classids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbnet.classids('accept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bend-45.2',\n",
       " 'build-26.1',\n",
       " 'coil-9.6',\n",
       " 'crane-40.3.2',\n",
       " 'preparing-26.3-1',\n",
       " 'roll-51.3.1',\n",
       " 'run-51.3.2-2',\n",
       " 'shake-22.3-2',\n",
       " 'slide-11.2-1',\n",
       " 'sound_emission-43.2',\n",
       " 'sound_existence-47.4',\n",
       " 'split-23.2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn.classids('roll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_31_2 = vn.vnclass('accept-77.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent [+animate] [+organization] Theme "
     ]
    }
   ],
   "source": [
    "for themrole in vn_31_2.findall('THEMROLES/THEMROLE'):\n",
    "    print(themrole.attrib['type'], end=' ')\n",
    "    for selrestr in themrole.findall('SELRESTRS/SELRESTR'):\n",
    "        print('[%(Value)s%(type)s]' %selrestr.attrib, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Theme "
     ]
    }
   ],
   "source": [
    "#get thematic roles from each vnclass\n",
    "for themrole in vn_31_2.findall('THEMROLES/THEMROLE'):\n",
    "    print(themrole.attrib['type'], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(verbnet.pprint('31.2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE\n",
    "# vn.classids(lemma='believe')\n",
    "# vn.classids(wordnetid='lead%2:38:01')\n",
    "# vn.classids(fileid='consider-29.9.xml')\n",
    "# vn.vnclass('admire-31.2')\n",
    "# vn.fileids('admire-31.2')\n",
    "# vn.longid('31.2')\n",
    "# vn.shortid('31.2')\n",
    "# vn.lemmas('29.9-1-1-1')\n",
    "#vn.themroles()\n",
    "# vn._index()\n",
    "# f = vn.vnclass('29.9').findall('FRAMES/FRAME')\n",
    "# vn.pprint_frames('29.9-1', indent='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vn.pprint_frames('29.9-1', indent='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the whole CREA ratings into crea_df\n",
    "with open('/Users/songheekim/Google Drive/Primary/Projects/VerbVector/CARratings_all.csv') as f:\n",
    "    crea_df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e3aaf6961da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get the column names of CREA and sort alphabetically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcrea_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrea_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcrea_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrea_attributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "#get the column names of CREA and sort alphabetically\n",
    "crea_attributes = crea_df.columns\n",
    "crea_attributes = sorted(crea_attributes.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crea_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset to only verbs\n",
    "crea_verb_df = crea_df[crea_df['WC']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_verb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of past tense verbs in CREA\n",
    "crea_verb_list = crea_verb_df['Word'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the list of original verbs and bare verbs \n",
    "#first construct a list of bare form verbs in CREA (manual entry)\n",
    "crea_verb_bare = ['approach', 'arrest', 'eat', 'block', 'buy', 'break', 'build', 'carry', 'celebrate', 'cross', \n",
    "                  'damage', 'deliver', 'destroy', 'drink', 'draw', 'drop', 'end', 'fear', 'feed', 'fix', \n",
    "                 'fly', 'find', 'give', 'grow', 'hold', 'help', 'hike', 'interview', 'kick', 'land', \n",
    "                 'laugh', 'leave', 'like', 'listen', 'live', 'lose', 'march', 'meet', 'negotiate', 'open',\n",
    "                 'plan', 'play', 'put', 'run', 'read', 'see', 'shout', 'sleep', 'speak', 'stay',\n",
    "                 'steal', 'survive', 'throw', 'take', 'use', 'visit', 'walk', 'want', 'watch', 'go',\n",
    "                 'work', 'write']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(crea_verb_list)):\n",
    "#     print (crea_verb_list[i], crea_verb_bare[i])\n",
    "# #the result looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert Bare verbs into crea_verb_df\n",
    "crea_verb_df.insert(2, \"Lemma\", crea_verb_bare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crea_verb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mark whether the word comes from CREA or VerbNet\n",
    "crea_verb_df.insert(0, \"Database\", \"CREA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the overlapping columns (i.e., TIME, DURATION, PATH are present in both VerbNet and CREA)\n",
    "crea_verb_df = crea_verb_df.rename(columns={'Time': 'TimeCREA', 'Duration': 'DurationCREA', 'Path': 'PathCREA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crea_verb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of verbs that are in VerbNet but not in CREA\n",
    "crea_verb_bare\n",
    "\n",
    "#[i for i in vn_lemma if i not in crea_verb_bare]\n",
    "new_verb = sorted(list(set(vn_lemma) - set(crea_verb_bare)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct another df which contains new verbs from VerbNet as rows and the same column names as in CREA_df\n",
    "verbnet_df = pd.DataFrame({'Lemma' : new_verb}, columns=crea_verb_df.columns)\n",
    "verbnet_df['Database'] = 'VerbNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbnet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine CREA df and VerbNet df\n",
    "all_df = crea_verb_df.append(verbnet_df, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of thematic roles from VerbNet \n",
    "classids = vn.classids()\n",
    "themroles = []\n",
    "for id in classids: \n",
    "    liszt = vn.themroles(id)\n",
    "    for i in range(len(liszt)):\n",
    "        themroles.append(liszt[i]['type'])\n",
    "\n",
    "print (len(set(themroles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thematic_roles = (set(themroles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of new columns to be added to all_df\n",
    "col_from_verbnet = sorted(list(set(themroles))) + all_semantic_preds\n",
    "len(col_from_verbnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following checks if column names from VerbNet already exist in crea_verb_df\n",
    "[x for x in crea_verb_df.columns if x in col_from_verbnet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add themroles to add_df as columns\n",
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df.reindex(columns=all_df.columns.to_list() + col_from_verbnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.set_index('Lemma', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_csv('/Users/songkim/Google Drive/Primary/Projects/VerbVector/verbvector_demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v3.classids('hit')\n",
    "\n",
    "# v3.fileids('bump-18.4')\n",
    "\n",
    "# v3.frames('bump-18.4')\n",
    "\n",
    "# v3.themroles(v3.vnclass(v3.fileids('hit-18.1')[0]))\n",
    "\n",
    "# v3.themroles(v3.vnclass(v3.classids('hit')[0]))\n",
    "\n",
    "# v3.fileids('reach-51.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether there is only one fileid for a given classid\n",
    "all_classid = vn.classids()\n",
    "for a in all_classid:\n",
    "    if len(vn.fileids(a)) != 1:\n",
    "        print (a, 'len of fileid not 1')\n",
    "\n",
    "#nothing prints out, confirming only one fileid for a classid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch themroles for each lemma (going to be removed)\n",
    "# themrole_dict = {}\n",
    "# for l in v3.lemmas():\n",
    "#     c_id = v3.classids(l)\n",
    "#     f_id = v3.fileids(c_id)[0]\n",
    "#     vnclass = v3.vnclass(f_id)\n",
    "#     themrole = v3.themroles(vnclass)\n",
    "#     if l in themrole_dict.keys():\n",
    "#         print (\"lemma alreay exists\")\n",
    "#     else: \n",
    "#         themrole_dict[l] = themrole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# c_id = 'reach-51.8'\n",
    "# f_id = vn.fileids(c_id)[0]\n",
    "# vnclass = vn.vnclass(f_id)\n",
    "# vn.themroles(vnclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classid_dict = {}\n",
    "# for i in v3.lemmas():\n",
    "#     c_id = v3.classids(i)\n",
    "#     classid_dict[i]=c_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to check what the next cell does\n",
    "# c_id = vn.classids('hit')\n",
    "# print ('c_id\\n', c_id)\n",
    "# ln = len(c_id)\n",
    "# for i in range(ln):\n",
    "#     f_id = vn.fileids(c_id[i])[0]\n",
    "#     vnclass = vn.vnclass(f_id)\n",
    "#     troles = vn.themroles(vnclass)\n",
    "#     print ('i is:', i, ', troles\\n', troles)\n",
    "##########IT SEEMS THAT vn.themroles does NOT fetch roles from its subclasses (of vnclass) SHOULD BE FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch themroles for each lemma (updated)\n",
    "themrole_dict = {}\n",
    "for l in vn.lemmas():\n",
    "    c_id = vn.classids(l)\n",
    "    ln = len(c_id)\n",
    "    roles = []\n",
    "    for j in range(ln): \n",
    "        f_id = vn.fileids(c_id[j])[0]\n",
    "        vnclass = vn.vnclass(f_id)\n",
    "        themrole = vn.themroles(vnclass)\n",
    "        roles += themrole\n",
    "    if l in themrole_dict.keys():\n",
    "        print (\"lemma already exists\")\n",
    "    else: \n",
    "        themrole_dict[l] = roles        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acquiesce-95.1',\n",
       " 'become-109.1-1',\n",
       " 'calibratable_cos-45.6.1-1',\n",
       " 'convert-26.6.2',\n",
       " 'die-42.4',\n",
       " 'escape-51.1-1',\n",
       " 'long-32.2-1',\n",
       " 'meander-47.7',\n",
       " 'occur-48.3',\n",
       " 'succeed-74-3']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn.classids('fall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Agent',\n",
       "  'modifiers': [{'value': '+', 'type': 'animate'},\n",
       "   {'value': '+', 'type': 'organization'}]},\n",
       " {'type': 'Theme', 'modifiers': []},\n",
       " {'type': 'Source', 'modifiers': []},\n",
       " {'type': 'Agent',\n",
       "  'modifiers': [{'value': '+', 'type': 'animate'},\n",
       "   {'value': '+', 'type': 'machine'}]},\n",
       " {'type': 'Material', 'modifiers': [{'value': '+', 'type': 'concrete'}]},\n",
       " {'type': 'Product', 'modifiers': []},\n",
       " {'type': 'Beneficiary',\n",
       "  'modifiers': [{'value': '+', 'type': 'animate'},\n",
       "   {'value': '+', 'type': 'organization'}]},\n",
       " {'type': 'Asset', 'modifiers': [{'value': '+', 'type': 'currency'}]},\n",
       " {'type': 'Patient', 'modifiers': []},\n",
       " {'type': 'Attribute', 'modifiers': []}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themrole_dict['build']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to calculate how many themroles the lemmas have \n",
    "length_range = []\n",
    "for value in themrole_dict.values():\n",
    "    l= len(value)\n",
    "    length_range.append(l)\n",
    "print (set(length_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_words = random.choices(vn_lemma, k=3)\n",
    "\n",
    "random_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in random_words:\n",
    "    print (w, themrole_dict[w], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all_df to dictionary\n",
    "all_dict = all_df.to_dict(orient='index')\n",
    "\n",
    "#and fill in zero for VERBNET features\n",
    "for key, value in all_dict.items():\n",
    "    for feature in col_from_verbnet: \n",
    "        value[feature] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the zero filled dict to df\n",
    "all_df_zero_filled = pd.DataFrame.from_dict(all_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_zero_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df_zero_filled.to_csv('Google Drive/Primary/Projects/VerbVector/verbvector_interim.csv')\n",
    "#all_df_filled should be removed ultimately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in random_words: \n",
    "#     print (len(themrole_dict[w]), themrole_dict[w], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('value =', themrole_dict['December']\n",
    "themrole_dict['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create another dictionary mapping a lemma to its themroles. ex {'inveigle: ['Agent', 'Patient', Predicate]'}\n",
    "themrole_dict_compact = {}\n",
    "for key, value in themrole_dict.items(): \n",
    "    n = len(value)\n",
    "    themrole_list = []\n",
    "    for i in range(n):\n",
    "        themrole_list.append(value[i]['type'])\n",
    "    themrole_dict_compact[key]=set(themrole_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#themrole_dict_compact['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_dict['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of Semantic Predicates used for each lemma\n",
    "def semantic_pred_from_lemma(self, lemma):\n",
    "    lemma_classids = self.classids(lemma)\n",
    "    pred_list_combined = []\n",
    "    for k in range(len(lemma_classids)):\n",
    "        vnclass = self.vnclass(lemma_classids[k])\n",
    "        frame_list = vnclass.findall('FRAMES/FRAME')\n",
    "        pred_list = []\n",
    "        for i in range(len(frame_list)):\n",
    "            semantics_per_frame = self._get_semantics_within_frame(frame_list[i])\n",
    "            for j in range(len(semantics_per_frame)):\n",
    "                pred = semantics_per_frame[j]['predicate_value']\n",
    "                pred_list.append(pred)\n",
    "        pred_list_combined.append(pred_list)\n",
    "    \n",
    "    pred_list_combined_final = list(set(list(flatten(pred_list_combined))))\n",
    "\n",
    "\n",
    "    return pred_list_combined_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic_pred_from_lemma(vn, 'hit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dict mapping a lemma to its semantic predicates, ex {'hit': ['motion','contact','harmed', 'cause', ...]}\n",
    "semantic_pred_dict = {}\n",
    "for lemma in vn.lemmas():\n",
    "    if lemma in semantic_pred_dict.keys():\n",
    "        print (lemma, \" already exists!\")\n",
    "    else: \n",
    "        semantic_pred_dict[lemma] = semantic_pred_from_lemma(vn, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic_pred_dict\n",
    "#all_dict['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update all_dict so that thematic role columns to be updated (0-->1) where appropriate\n",
    "for k, v in all_dict.items():\n",
    "    themroles = themrole_dict_compact[k]\n",
    "    for t in themroles:\n",
    "        v[t] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update all dict so that Semantic Predicate columns to be updated (0-->1)where appropriate\n",
    "for q, w in all_dict.items():\n",
    "    semantic_preds = semantic_pred_dict[q]\n",
    "    for s in semantic_preds: \n",
    "        w[s] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict['eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_final = pd.DataFrame.from_dict(all_dict, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add two columns: 'Aspect', 'SynClass'\n",
    "all_df_final.insert(1, 'AspClass', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_final.insert(2, 'SynClass', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df_final is a df with both CREA AND VERBNET entries and all the column names (except SUBTLEX frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_final.to_csv('/users/songkim/Google Drive/Primary/Projects/VerbVector/verbvec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict_final = all_df_final.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether cell values for thematic roles are accurate\n",
    "themrole_from_csv = {}\n",
    "for key, val in all_dict_final.items():\n",
    "    thlist_for_each_key = []\n",
    "    for key2, val2 in val.items():\n",
    "        if val2 == 1 and key2 in thematic_roles:\n",
    "            thlist_for_each_key.append(key2)\n",
    "    themrole_from_csv[key] = thlist_for_each_key\n",
    "    \n",
    "#all_dict['hit'] = {'Database':'CREA', 'Agent':1, 'utilize':nan, 'cause':1...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chec\n",
    "themrole_from_csv['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(themrole_dict_compact['hit'])) == themrole_from_csv['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in vn.lemmas():\n",
    "    if themrole_from_csv[w] != sorted(list(themrole_dict_compact[w])):\n",
    "        print (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open SUBTLEXus text file, search for 'null' and replace with 'null_ignore' (otherwise raising error)\n",
    "\n",
    "rows = []\n",
    "with open('/users/songkim/Google Drive/Primary/Projects/VerbVector/SUBTLEX-US_frequency_PoS.txt') as f:\n",
    "    i = 0\n",
    "    for row in csv.reader(f, delimiter ='\\t'):\n",
    "        #print (row)\n",
    "        #i += 1\n",
    "        if row[0] == 'null':\n",
    "            row[0] = 'null_ignore'\n",
    "            rows.append(row)\n",
    "            print (row)\n",
    "        else:\n",
    "            rows.append(row)\n",
    "\n",
    "with open('/users/songkim/Google Drive/Primary/Projects/VerbVector/SUBTLEX-US_frequency_PoS_fixed.csv', 'w') as g: \n",
    "    writer = csv.writer(g, delimiter = '\\t')\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublex = pd.read_csv('/users/songkim/Google Drive/Primary/Projects/VerbVector/SUBTLEX-US_frequency_PoS_fixed.csv', delimiter = '\\t', index_col='Word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous checks (no longer relevant)\n",
    "# sublex[sublex['Word'].duplicated()]\n",
    "# sublex[sublex['Word'].isin(['null'])]\n",
    "# sublex['Word']\n",
    "# 'null' in list(sublex['Word'])\n",
    "# sublex['Word'].replace('null', 'null_ignore', inplace=True)\n",
    "# 'zygoma' in list(sublex['Word'])\n",
    "# sublex[sublex['Word'].isin(['null_ignore'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublex_dict = sublex.to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether the words are unique (not if 'null' was not replaced with 'null_ignore')\n",
    "sublex.index.is_unique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublex_dict['escape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sublex_dict['December']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublex_lemma = list(sublex_dict.keys())\n",
    "#sublex_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(all_dict['escape'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(all_dict_final['escape'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_dict_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in all_dict_final.items():\n",
    "    if key in sublex_lemma:\n",
    "        value['FREQcount'] = sublex_dict[key]['FREQcount']\n",
    "        value['Dom_PoS_SUBTLEX'] = sublex_dict[key]['Dom_PoS_SUBTLEX']\n",
    "        value['All_PoS_SUBTLEX'] = sublex_dict[key]['All_PoS_SUBTLEX']\n",
    "        value['Percentage_dom_PoS'] = sublex_dict[key]['Percentage_dom_PoS']\n",
    "    else:\n",
    "        value['FREQcount'] = ''\n",
    "        value['Dom_PoS_SUBTLEX'] = ''\n",
    "        value['All_PoS_SUBTLEX'] = ''\n",
    "        value['Percentage_dom_PoS'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_dict_final['December']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose only those words whose dominant PoS is verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_w_frequency = pd.DataFrame.from_dict(all_dict_final, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_w_frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN this line to get the df for ALL CREA verbs (regardless of their dominant part of speech)\n",
    "crea_all = all_df_w_frequency[all_df_w_frequency['Database']=='CREA']\n",
    "crea_all.shape\n",
    "crea_all.to_csv('/users/songkim/Google Drive/Primary/Projects/VerbVector/crea_vratings_subtlex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose only those words whose dominant PoS is verb\n",
    "verb_dominant_df_w_frequency = all_df_w_frequency[all_df_w_frequency['Dom_PoS_SUBTLEX']=='Verb']\n",
    "verb_dominant_df_w_frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN this line to get the df for all CREA verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_w_frequency.to_csv('/users/songkim/Google Drive/Primary/Projects/VerbVector/verbvec_frequency.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_dominant_df_w_frequency.to_csv('/users/songkim/Google Drive/Primary/Projects/VerbVector/verbvec_verbdominant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_dominant_df_w_frequency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_w_frequency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#write a file contains all the verb entries (input to CELEX)\n",
    "verblist = verb_dominant_df_w_frequency.index.to_list()\n",
    "with open('/users/songkim/Google Drive/Primary/Projects/VerbVector/2200verbs.txt', 'w') as t:\n",
    "    for word in verblist:\n",
    "        t.write('%s\\n' %word)\n",
    "\n",
    "with open('/users/songkim/Google Drive/Primary/Projects/VerbVector/celex_2200_Head,Class,Cob,Trans_V,Intrans_V,Ditrans_V') as c:\n",
    "    raw_celex = pd.read_csv(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find verbs with more than one entry in celex (polysemy) and remove redundancy\n",
    "celex_verb = raw_celex['Head'].to_list()\n",
    "multiple_entry_verb = set([x for x in celex_verb if celex_verb.count(x) > 1])\n",
    "\n",
    "#print only the verbs with multiple entries in celex result\n",
    "raw_celex.loc[raw_celex['Head'].isin(multiple_entry_verb)]\n",
    "\n",
    "#delete one of the duplicate rows\n",
    "raw_celex.drop([224, 354, 616, 810, 891, 899, 912, 1093, 1125, 1420, 1435, 1469, 1638, 1724, 1731, 1963, 2014, 2036], inplace=True)\n",
    "\n",
    "# verblist2 = raw_celex['Head'].to_list()\n",
    "# len(verblist2)\n",
    "\n",
    "#len([x for x in verblist if x not in raw_celex['Head'].to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add CELEX columns to the verb_dominant_df_w_frequency\n",
    "\n",
    "raw_celex.set_index('Head', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_dominant_df_w_frequency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_celex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_union = pd.concat([raw_celex, verb_dominant_df_w_frequency], axis=1, join = 'outer', sort='False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_intersection = pd.concat([raw_celex, verb_dominant_df_w_frequency], axis=1, join = 'inner', sort='False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = combined_intersection.to_dict(orient='index')\n",
    "combined_dict['eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dict = {'a': {'Class': 'V',\n",
    " 'Cob': 975,\n",
    " 'Trans_V': 'Y',\n",
    " 'Intrans_V': 'N',\n",
    " 'Ditrans_V': 'N',\n",
    " 'Database': 'VerbNet',\n",
    " 'AspClass': '',\n",
    " 'SynClass': '',\n",
    " 'No': 'nan',\n",
    " 'Word': 'nan',\n",
    " 'WC': 'nan',\n",
    " 'ID': 'nan',\n",
    " 'KRNS': 'nan',\n",
    " 'N': 'nan'}, 'b':{'Class': 'V',\n",
    " 'Cob': 5183,\n",
    " 'Trans_V': 'Y',\n",
    " 'Intrans_V': 'Y',\n",
    " 'Ditrans_V': 'N',\n",
    " 'Database': 'CREA',\n",
    " 'AspClass': '',\n",
    " 'SynClass': '',\n",
    " 'No': 183.0,\n",
    " 'Word': 'ate',\n",
    " 'WC': 2.0,\n",
    " 'ID': 245.0,\n",
    " 'KRNS': 'Y',\n",
    " 'N': 29.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in toy_dict.items():\n",
    "    print (v['Trans_V'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in combined_dict.items(): \n",
    "    if (v['Trans_V'] == 'Y') and (v['Intrans_V']=='N'):\n",
    "        v['SynClass'] = 'transitive'\n",
    "    elif (v['Trans_V'] == 'N') and (v['Intrans_V']=='Y'):\n",
    "        v['SynClass'] = 'intransitive'\n",
    "    elif (v['Trans_V'] == 'Y') and (v['Intrans_V']=='Y'):\n",
    "        v['SynClass'] = 'both'\n",
    "    else:\n",
    "        v['SynClass'] = 'neither'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_w_celex is a dataframe that includes crea+verbnet+frequency info from SUBTLEX + CELEX info\n",
    "df_w_celex = pd.DataFrame.from_dict(combined_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = (df_w_celex.SynClass == 'transitive').sum()\n",
    "intrans = (df_w_celex.SynClass == 'intransitive').sum()\n",
    "both = (df_w_celex.SynClass == 'both').sum()\n",
    "neither = (df_w_celex.SynClass == 'neither').sum()\n",
    "total = combined_intersection.shape[0]\n",
    "\n",
    "print ('# of transitive verbs is ', trans, ' out of %s' %total)\n",
    "print ('# of intransitive verbs is ', intrans, ' out of %s' %total)\n",
    "print ('# of verbs that can be both transitive and intransitive is', both, ' out of %s' %total)\n",
    "print ('# of verbs that are neither used as trans or intrans is', neither, ' out of %s' %total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
