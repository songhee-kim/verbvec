{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import verbnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn = nltk.corpus.util.LazyCorpusLoader('verbnet3', nltk.corpus.reader.verbnet.VerbnetCorpusReader, r'(?!\\.).*\\.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of verbs in Verbnet\n",
    "#vn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_lemma = vn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from_stack_over_flow\n",
    "from collections import Iterable                           \n",
    "\n",
    "def flatten(items):\n",
    "    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vn.frames('hit-18.1-1')\n",
    "#vn.frames('hit-18.1-1')[0]['semantics'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vn.frames('hit-18.1-1')[0]['semantics'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vn.vnclass('hit-18.1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vn.vnclass('hit-18.1-1').findall('FRAMES/FRAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vn.vnclass('hit-18.1-1').findall('FRAMES/FRAME')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vn._get_semantics_within_frame(vn.vnclass('hit-18.1-1').findall('FRAMES/FRAME')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = vn._get_semantics_within_frame(vn.vnclass('hit-18.1-1').findall('FRAMES/FRAME')[0])\n",
    "# pred_list = []\n",
    "# for i in range(len(preds)):\n",
    "#     pred_list.append(preds[i]['predicate_value'])\n",
    "# pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_list = vn.vnclass('hit-18.1-1').findall('FRAMES/FRAME')\n",
    "# pred_list = []\n",
    "# for i in range(len(frame_list)):\n",
    "#     semantics_per_frame = vn._get_semantics_within_frame(frame_list[i])\n",
    "#     for j in range(len(semantics_per_frame)):\n",
    "#         pred = semantics_per_frame[j]['predicate_value']\n",
    "#         pred_list.append(pred)\n",
    "# pred_list = set(pred_list)\n",
    "# print(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of Semantic Predicates given verbnet class (such as 'hit-18.1-1')\n",
    "def semantic_pred_from_class(self, vnclass):\n",
    "    #vnclass is the outcome of self.vnclass(classids)\n",
    "    frame_list = vnclass.findall('FRAMES/FRAME')\n",
    "    pred_list = []\n",
    "    for i in range(len(frame_list)):\n",
    "        semantics_per_frame = self._get_semantics_within_frame(frame_list[i])\n",
    "        for j in range(len(semantics_per_frame)):\n",
    "            pred = semantics_per_frame[j]['predicate_value']\n",
    "            pred_list.append(pred)\n",
    "    pred_list = list(set(pred_list))\n",
    "    \n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manner', 'contact', 'utilize', 'cause']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "semantic_pred_from_class(vn, vn.vnclass('hit-18.1-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absorb-39.8',\n",
       " 'accept-77.1',\n",
       " 'accompany-51.7',\n",
       " 'acquiesce-95.1',\n",
       " 'acquiesce-95.1-1']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a complete list of classids in VerbNet3.3\n",
    "all_classids = vn.classids()\n",
    "all_classids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "#make a complete list of Semantic Predicates used in VerbNet3.3 (from classids)\n",
    "interim_semantic_preds = []\n",
    "for ci in all_classids: \n",
    "    pred_per_class = semantic_pred_from_class(vn, vn.vnclass(ci))\n",
    "    interim_semantic_preds.append(pred_per_class)\n",
    "all_semantic_preds = sorted(list(set(list(flatten(interim_semantic_preds)))))\n",
    "print(len(all_semantic_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_semantic_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vn.classids('put')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic_pred_from_class(vn, vn.vnclass('put-9.1-2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test of 'flatten' function\n",
    "# l = [['assess'],\n",
    "#  ['transfer',\n",
    "#   'has_possession',\n",
    "#   'financial_interest_in',\n",
    "#   'cause',\n",
    "#   'transfer',\n",
    "#   'has_possession',\n",
    "#   'financial_interest_in',\n",
    "#   'cause'],\n",
    "#  ['motion', 'path_rel', 'path_rel', 'cause']]\n",
    "# l2 = [1,2,3,4,5,6]\n",
    "# list(flatten(l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of Semantic Predicates used for each lemma\n",
    "def semantic_pred_from_lemma(self, lemma):\n",
    "    lemma_classids = self.classids(lemma)\n",
    "    pred_list_combined = []\n",
    "    for k in range(len(lemma_classids)):\n",
    "        vnclass = self.vnclass(lemma_classids[k])\n",
    "        frame_list = vnclass.findall('FRAMES/FRAME')\n",
    "        pred_list = []\n",
    "        for i in range(len(frame_list)):\n",
    "            semantics_per_frame = self._get_semantics_within_frame(frame_list[i])\n",
    "            for j in range(len(semantics_per_frame)):\n",
    "                pred = semantics_per_frame[j]['predicate_value']\n",
    "                pred_list.append(pred)\n",
    "        pred_list_combined.append(pred_list)\n",
    "    \n",
    "    pred_list_combined_final = list(set(list(flatten(pred_list_combined))))\n",
    "\n",
    "\n",
    "    return pred_list_combined_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has_possession',\n",
       " 'path_rel',\n",
       " 'assess',\n",
       " 'financial_interest_in',\n",
       " 'motion',\n",
       " 'cause',\n",
       " 'transfer']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare the results of the following two: \n",
    "semantic_pred_from_lemma(vn, 'put')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### do it later \n",
    "#semantic_pred_from_class(vn, vn.vnclass(vn.classids('put')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst = [1, 2, 3, 4]\n",
    "# lst.append(5) if 5 not in lst else lst\n",
    "\n",
    "# lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbnet.classids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(verbnet.classids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(vn.classids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbnet_lemma = verbnet.lemmas()\n",
    "vn_lemma = vn.lemmas()\n",
    "#this shows that verbnet and vn load different datasets! Stick to vn, not verbnet (IMPORTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbnet_lemma\n",
    "#vn_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x for x in verbnet_lemma if x not in vn_lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x for x in vn.classids() if x not in verbnet.classids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbnet.classids('accept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accept-77.1', 'characterize-29.2-1-1', 'obtain-13.5.2']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn.classids('accept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_31_2 = vn.vnclass('accept-77.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for themrole in vn_31_2.findall('THEMROLES/THEMROLE'):\n",
    "#     print(themrole.attrib['type'], end=' ')\n",
    "#     for selrestr in themrole.findall('SELRESTRS/SELRESTR'):\n",
    "#         print('[%(Value)s%(type)s]' %selrestr.attrib, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Theme "
     ]
    }
   ],
   "source": [
    "#get thematic roles from each vnclass\n",
    "for themrole in vn_31_2.findall('THEMROLES/THEMROLE'):\n",
    "    print(themrole.attrib['type'], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(verbnet.pprint('31.2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE\n",
    "# vn.classids(lemma='believe')\n",
    "# vn.classids(wordnetid='lead%2:38:01')\n",
    "# vn.classids(fileid='consider-29.9.xml')\n",
    "# vn.vnclass('admire-31.2')\n",
    "# vn.fileids('admire-31.2')\n",
    "# vn.longid('31.2')\n",
    "# vn.shortid('31.2')\n",
    "# vn.lemmas('29.9-1-1-1')\n",
    "#vn.themroles()\n",
    "# vn._index()\n",
    "# f = vn.vnclass('29.9').findall('FRAMES/FRAME')\n",
    "# vn.pprint_frames('29.9-1', indent='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vn.pprint_frames('29.9-1', indent='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the whole CREA ratings into crea_df\n",
    "with open('/Users/songkim/Google Drive/Primary/Projects/VerbVector/CARratings_all.csv') as f:\n",
    "    crea_df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Word</th>\n",
       "      <th>WC</th>\n",
       "      <th>ID</th>\n",
       "      <th>KRNS</th>\n",
       "      <th>N</th>\n",
       "      <th>Mean R</th>\n",
       "      <th>Vision</th>\n",
       "      <th>Bright</th>\n",
       "      <th>Dark</th>\n",
       "      <th>...</th>\n",
       "      <th>Orth</th>\n",
       "      <th>Orth_F</th>\n",
       "      <th>N1_F</th>\n",
       "      <th>N1_C</th>\n",
       "      <th>N2_F</th>\n",
       "      <th>N2_C</th>\n",
       "      <th>N3_F</th>\n",
       "      <th>N3_C</th>\n",
       "      <th>Unnamed: 84</th>\n",
       "      <th>IMG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>accident</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "      <td>Y</td>\n",
       "      <td>30</td>\n",
       "      <td>0.7947</td>\n",
       "      <td>3.3000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3586.45</td>\n",
       "      <td>833.88</td>\n",
       "      <td>500.79</td>\n",
       "      <td>71.00</td>\n",
       "      <td>213.37</td>\n",
       "      <td>20.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>385</td>\n",
       "      <td>accordion</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>N</td>\n",
       "      <td>29</td>\n",
       "      <td>0.8298</td>\n",
       "      <td>5.5517</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2773.70</td>\n",
       "      <td>700.11</td>\n",
       "      <td>741.32</td>\n",
       "      <td>96.88</td>\n",
       "      <td>358.08</td>\n",
       "      <td>34.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.9367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>activist</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>Y</td>\n",
       "      <td>30</td>\n",
       "      <td>0.7987</td>\n",
       "      <td>4.6333</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3628.95</td>\n",
       "      <td>858.00</td>\n",
       "      <td>436.24</td>\n",
       "      <td>122.14</td>\n",
       "      <td>104.57</td>\n",
       "      <td>16.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>actor</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>Y</td>\n",
       "      <td>29</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>4.8276</td>\n",
       "      <td>0.5517</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5416.33</td>\n",
       "      <td>190.40</td>\n",
       "      <td>220.51</td>\n",
       "      <td>12.00</td>\n",
       "      <td>71.19</td>\n",
       "      <td>2.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>454</td>\n",
       "      <td>advantage</td>\n",
       "      <td>1</td>\n",
       "      <td>469</td>\n",
       "      <td>N</td>\n",
       "      <td>25</td>\n",
       "      <td>0.7771</td>\n",
       "      <td>0.7600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2180.43</td>\n",
       "      <td>565.89</td>\n",
       "      <td>333.04</td>\n",
       "      <td>53.62</td>\n",
       "      <td>93.42</td>\n",
       "      <td>11.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>517</td>\n",
       "      <td>year</td>\n",
       "      <td>1</td>\n",
       "      <td>532</td>\n",
       "      <td>N</td>\n",
       "      <td>29</td>\n",
       "      <td>0.8215</td>\n",
       "      <td>0.3793</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>68.00</td>\n",
       "      <td>13699.78</td>\n",
       "      <td>165.50</td>\n",
       "      <td>1964.93</td>\n",
       "      <td>33.00</td>\n",
       "      <td>914.82</td>\n",
       "      <td>7.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>35</td>\n",
       "      <td>yellow</td>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>Y</td>\n",
       "      <td>28</td>\n",
       "      <td>0.7847</td>\n",
       "      <td>4.7500</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>21.42</td>\n",
       "      <td>4385.01</td>\n",
       "      <td>385.17</td>\n",
       "      <td>576.83</td>\n",
       "      <td>54.20</td>\n",
       "      <td>196.10</td>\n",
       "      <td>16.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.2167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>29</td>\n",
       "      <td>young</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>Y</td>\n",
       "      <td>27</td>\n",
       "      <td>0.7447</td>\n",
       "      <td>5.0370</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8936.52</td>\n",
       "      <td>280.20</td>\n",
       "      <td>3696.99</td>\n",
       "      <td>41.75</td>\n",
       "      <td>909.68</td>\n",
       "      <td>7.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>493</td>\n",
       "      <td>zone</td>\n",
       "      <td>1</td>\n",
       "      <td>508</td>\n",
       "      <td>N</td>\n",
       "      <td>25</td>\n",
       "      <td>0.6678</td>\n",
       "      <td>2.5600</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>93.09</td>\n",
       "      <td>17724.14</td>\n",
       "      <td>267.75</td>\n",
       "      <td>1163.48</td>\n",
       "      <td>25.00</td>\n",
       "      <td>430.16</td>\n",
       "      <td>5.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>419</td>\n",
       "      <td>zoo</td>\n",
       "      <td>1</td>\n",
       "      <td>434</td>\n",
       "      <td>N</td>\n",
       "      <td>30</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>5.2667</td>\n",
       "      <td>1.1667</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>152.23</td>\n",
       "      <td>11509.49</td>\n",
       "      <td>51.67</td>\n",
       "      <td>542.01</td>\n",
       "      <td>4.50</td>\n",
       "      <td>9.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.3333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>535 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      No       Word  WC   ID KRNS   N  Mean R  Vision  Bright    Dark  ...  \\\n",
       "0     40   accident   1  191    Y  30  0.7947  3.3000  0.2000  0.6667  ...   \n",
       "1    385  accordion   1  400    N  29  0.8298  5.5517  0.5862  0.9310  ...   \n",
       "2    114   activist   1  143    Y  30  0.7987  4.6333  0.1667  0.2333  ...   \n",
       "3    115      actor   1   56    Y  29  0.7994  4.8276  0.5517  0.4828  ...   \n",
       "4    454  advantage   1  469    N  25  0.7771  0.7600  0.0000  0.0000  ...   \n",
       "..   ...        ...  ..  ...  ...  ..     ...     ...     ...     ...  ...   \n",
       "530  517       year   1  532    N  29  0.8215  0.3793  0.0690  0.0345  ...   \n",
       "531   35     yellow   3  103    Y  28  0.7847  4.7500  4.0000  0.4286  ...   \n",
       "532   29      young   3   88    Y  27  0.7447  5.0370  0.5556  0.0000  ...   \n",
       "533  493       zone   1  508    N  25  0.6678  2.5600  0.4400  0.4800  ...   \n",
       "534  419        zoo   1  434    N  30  0.7828  5.2667  1.1667  0.7000  ...   \n",
       "\n",
       "     Orth  Orth_F      N1_F    N1_C     N2_F    N2_C    N3_F   N3_C  \\\n",
       "0       1    0.06   3586.45  833.88   500.79   71.00  213.37  20.00   \n",
       "1       0    0.00   2773.70  700.11   741.32   96.88  358.08  34.14   \n",
       "2       0    0.00   3628.95  858.00   436.24  122.14  104.57  16.00   \n",
       "3       0    0.00   5416.33  190.40   220.51   12.00   71.19   2.33   \n",
       "4       0    0.00   2180.43  565.89   333.04   53.62   93.42  11.71   \n",
       "..    ...     ...       ...     ...      ...     ...     ...    ...   \n",
       "530    13   68.00  13699.78  165.50  1964.93   33.00  914.82   7.50   \n",
       "531     3   21.42   4385.01  385.17   576.83   54.20  196.10  16.00   \n",
       "532     0    0.00   8936.52  280.20  3696.99   41.75  909.68   7.67   \n",
       "533     9   93.09  17724.14  267.75  1163.48   25.00  430.16   5.50   \n",
       "534     7  152.23  11509.49   51.67   542.01    4.50    9.22   1.00   \n",
       "\n",
       "     Unnamed: 84     IMG  \n",
       "0            NaN  5.1800  \n",
       "1            NaN  5.9367  \n",
       "2            NaN     NaN  \n",
       "3            NaN  6.1000  \n",
       "4            NaN  2.9133  \n",
       "..           ...     ...  \n",
       "530          NaN  4.1100  \n",
       "531          NaN  6.2167  \n",
       "532          NaN  5.2600  \n",
       "533          NaN  4.3267  \n",
       "534          NaN  6.3333  \n",
       "\n",
       "[535 rows x 86 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the column names of CREA and sort alphabetically\n",
    "crea_attributes = crea_df.columns\n",
    "crea_attributes = sorted(crea_attributes.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crea_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset to only verbs\n",
    "crea_verb_df = crea_df[crea_df['WC']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Word</th>\n",
       "      <th>WC</th>\n",
       "      <th>ID</th>\n",
       "      <th>KRNS</th>\n",
       "      <th>N</th>\n",
       "      <th>Mean R</th>\n",
       "      <th>Vision</th>\n",
       "      <th>Bright</th>\n",
       "      <th>Dark</th>\n",
       "      <th>...</th>\n",
       "      <th>Orth</th>\n",
       "      <th>Orth_F</th>\n",
       "      <th>N1_F</th>\n",
       "      <th>N1_C</th>\n",
       "      <th>N2_F</th>\n",
       "      <th>N2_C</th>\n",
       "      <th>N3_F</th>\n",
       "      <th>N3_C</th>\n",
       "      <th>Unnamed: 84</th>\n",
       "      <th>IMG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>181</td>\n",
       "      <td>approached</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>Y</td>\n",
       "      <td>27</td>\n",
       "      <td>0.7827</td>\n",
       "      <td>3.0741</td>\n",
       "      <td>0.2593</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>18.32</td>\n",
       "      <td>1663.80</td>\n",
       "      <td>615.60</td>\n",
       "      <td>411.38</td>\n",
       "      <td>124.33</td>\n",
       "      <td>93.14</td>\n",
       "      <td>19.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>182</td>\n",
       "      <td>arrested</td>\n",
       "      <td>2</td>\n",
       "      <td>201</td>\n",
       "      <td>Y</td>\n",
       "      <td>30</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>1.9667</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5416.60</td>\n",
       "      <td>1143.25</td>\n",
       "      <td>1515.17</td>\n",
       "      <td>274.14</td>\n",
       "      <td>301.04</td>\n",
       "      <td>55.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>183</td>\n",
       "      <td>ate</td>\n",
       "      <td>2</td>\n",
       "      <td>245</td>\n",
       "      <td>Y</td>\n",
       "      <td>29</td>\n",
       "      <td>0.7808</td>\n",
       "      <td>2.5862</td>\n",
       "      <td>0.2759</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>504.00</td>\n",
       "      <td>38891.36</td>\n",
       "      <td>38.00</td>\n",
       "      <td>42.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>42.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.7067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>184</td>\n",
       "      <td>blocked</td>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>Y</td>\n",
       "      <td>27</td>\n",
       "      <td>0.6496</td>\n",
       "      <td>3.2963</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>2.2963</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.99</td>\n",
       "      <td>6085.64</td>\n",
       "      <td>1021.86</td>\n",
       "      <td>1588.17</td>\n",
       "      <td>268.17</td>\n",
       "      <td>126.68</td>\n",
       "      <td>34.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>185</td>\n",
       "      <td>bought</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>Y</td>\n",
       "      <td>29</td>\n",
       "      <td>0.7225</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.05</td>\n",
       "      <td>4891.08</td>\n",
       "      <td>473.83</td>\n",
       "      <td>846.64</td>\n",
       "      <td>53.60</td>\n",
       "      <td>331.92</td>\n",
       "      <td>14.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>241</td>\n",
       "      <td>wanted</td>\n",
       "      <td>2</td>\n",
       "      <td>219</td>\n",
       "      <td>Y</td>\n",
       "      <td>26</td>\n",
       "      <td>0.7159</td>\n",
       "      <td>1.6154</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>11.10</td>\n",
       "      <td>10154.81</td>\n",
       "      <td>1033.00</td>\n",
       "      <td>3113.97</td>\n",
       "      <td>297.00</td>\n",
       "      <td>563.92</td>\n",
       "      <td>43.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>234</td>\n",
       "      <td>watched</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>Y</td>\n",
       "      <td>29</td>\n",
       "      <td>0.7017</td>\n",
       "      <td>5.8621</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.7931</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4.40</td>\n",
       "      <td>6718.97</td>\n",
       "      <td>1137.71</td>\n",
       "      <td>2173.33</td>\n",
       "      <td>313.50</td>\n",
       "      <td>395.70</td>\n",
       "      <td>58.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>235</td>\n",
       "      <td>went</td>\n",
       "      <td>2</td>\n",
       "      <td>207</td>\n",
       "      <td>Y</td>\n",
       "      <td>31</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>1.4194</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>93.33</td>\n",
       "      <td>21234.43</td>\n",
       "      <td>228.25</td>\n",
       "      <td>3338.50</td>\n",
       "      <td>27.33</td>\n",
       "      <td>1131.46</td>\n",
       "      <td>6.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>236</td>\n",
       "      <td>worked</td>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>Y</td>\n",
       "      <td>28</td>\n",
       "      <td>0.7647</td>\n",
       "      <td>2.8929</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>8.35</td>\n",
       "      <td>9936.73</td>\n",
       "      <td>975.83</td>\n",
       "      <td>2660.75</td>\n",
       "      <td>261.40</td>\n",
       "      <td>575.87</td>\n",
       "      <td>34.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.8250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>237</td>\n",
       "      <td>wrote</td>\n",
       "      <td>2</td>\n",
       "      <td>209</td>\n",
       "      <td>Y</td>\n",
       "      <td>29</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>2.6897</td>\n",
       "      <td>0.3793</td>\n",
       "      <td>0.3793</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>56.67</td>\n",
       "      <td>11480.10</td>\n",
       "      <td>380.40</td>\n",
       "      <td>1107.29</td>\n",
       "      <td>45.75</td>\n",
       "      <td>201.20</td>\n",
       "      <td>4.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.3067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      No        Word  WC   ID KRNS   N  Mean R  Vision  Bright    Dark  ...  \\\n",
       "18   181  approached   2   19    Y  27  0.7827  3.0741  0.2593  0.0741  ...   \n",
       "22   182    arrested   2  201    Y  30  0.7622  1.9667  0.3000  1.1000  ...   \n",
       "25   183         ate   2  245    Y  29  0.7808  2.5862  0.2759  0.0690  ...   \n",
       "53   184     blocked   2  187    Y  27  0.6496  3.2963  0.4444  2.2963  ...   \n",
       "59   185      bought   2   23    Y  29  0.7225  3.0000  0.2414  0.1379  ...   \n",
       "..   ...         ...  ..  ...  ...  ..     ...     ...     ...     ...  ...   \n",
       "509  241      wanted   2  219    Y  26  0.7159  1.6154  0.1154  0.0385  ...   \n",
       "510  234     watched   2   13    Y  29  0.7017  5.8621  1.0690  0.7931  ...   \n",
       "513  235        went   2  207    Y  31  0.7725  1.4194  0.0323  0.2258  ...   \n",
       "525  236      worked   2   78    Y  28  0.7647  2.8929  0.3214  0.1429  ...   \n",
       "528  237       wrote   2  209    Y  29  0.7577  2.6897  0.3793  0.3793  ...   \n",
       "\n",
       "     Orth  Orth_F      N1_F     N1_C     N2_F    N2_C     N3_F   N3_C  \\\n",
       "18      1   18.32   1663.80   615.60   411.38  124.33    93.14  19.50   \n",
       "22      1    0.00   5416.60  1143.25  1515.17  274.14   301.04  55.50   \n",
       "25      9  504.00  38891.36    38.00    42.83    1.00    42.83   1.00   \n",
       "53      3    0.99   6085.64  1021.86  1588.17  268.17   126.68  34.00   \n",
       "59      4   20.05   4891.08   473.83   846.64   53.60   331.92  14.75   \n",
       "..    ...     ...       ...      ...      ...     ...      ...    ...   \n",
       "509     8   11.10  10154.81  1033.00  3113.97  297.00   563.92  43.50   \n",
       "510     6    4.40   6718.97  1137.71  2173.33  313.50   395.70  58.60   \n",
       "513    18   93.33  21234.43   228.25  3338.50   27.33  1131.46   6.50   \n",
       "525     5    8.35   9936.73   975.83  2660.75  261.40   575.87  34.00   \n",
       "528     2   56.67  11480.10   380.40  1107.29   45.75   201.20   4.67   \n",
       "\n",
       "     Unnamed: 84     IMG  \n",
       "18           NaN  3.5950  \n",
       "22           NaN  6.2000  \n",
       "25           NaN  3.7067  \n",
       "53           NaN     NaN  \n",
       "59           NaN  3.2000  \n",
       "..           ...     ...  \n",
       "509          NaN  4.0000  \n",
       "510          NaN  3.0000  \n",
       "513          NaN  2.5667  \n",
       "525          NaN  4.8250  \n",
       "528          NaN  4.3067  \n",
       "\n",
       "[62 rows x 86 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crea_verb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of past tense verbs in CREA\n",
    "crea_verb_list = crea_verb_df['Word'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the list of original verbs and bare verbs \n",
    "#first construct a list of bare form verbs in CREA (manual entry)\n",
    "crea_verb_bare = ['approach', 'arrest', 'eat', 'block', 'buy', 'break', 'build', 'carry', 'celebrate', 'cross', \n",
    "                  'damage', 'deliver', 'destroy', 'drink', 'draw', 'drop', 'end', 'fear', 'feed', 'fix', \n",
    "                 'fly', 'find', 'give', 'grow', 'hold', 'help', 'hike', 'interview', 'kick', 'land', \n",
    "                 'laugh', 'leave', 'like', 'listen', 'live', 'lose', 'march', 'meet', 'negotiate', 'open',\n",
    "                 'plan', 'play', 'put', 'run', 'read', 'see', 'shout', 'sleep', 'speak', 'stay',\n",
    "                 'steal', 'survive', 'throw', 'take', 'use', 'visit', 'walk', 'want', 'watch', 'go',\n",
    "                 'work', 'write']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(crea_verb_list)):\n",
    "#     print (crea_verb_list[i], crea_verb_bare[i])\n",
    "# #the result looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert Bare verbs into crea_verb_df\n",
    "crea_verb_df.insert(2, \"Lemma\", crea_verb_bare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crea_verb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mark whether the word comes from CREA or VerbNet\n",
    "crea_verb_df.insert(0, \"Database\", \"CREA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the overlapping columns (i.e., TIME, DURATION, PATH are present in both VerbNet and CREA)\n",
    "crea_verb_df = crea_verb_df.rename(columns={'Time': 'TimeCREA', 'Duration': 'DurationCREA', 'Path': 'PathCREA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crea_verb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of verbs that are in VerbNet but not in CREA\n",
    "crea_verb_bare\n",
    "\n",
    "#[i for i in vn_lemma if i not in crea_verb_bare]\n",
    "new_verb = sorted(list(set(vn_lemma) - set(crea_verb_bare)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct another df which contains new verbs from VerbNet as rows and the same column names as in CREA_df\n",
    "verbnet_df = pd.DataFrame({'Lemma' : new_verb}, columns=crea_verb_df.columns)\n",
    "verbnet_df['Database'] = 'VerbNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>No</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>WC</th>\n",
       "      <th>ID</th>\n",
       "      <th>KRNS</th>\n",
       "      <th>N</th>\n",
       "      <th>Mean R</th>\n",
       "      <th>Vision</th>\n",
       "      <th>...</th>\n",
       "      <th>Orth</th>\n",
       "      <th>Orth_F</th>\n",
       "      <th>N1_F</th>\n",
       "      <th>N1_C</th>\n",
       "      <th>N2_F</th>\n",
       "      <th>N2_C</th>\n",
       "      <th>N3_F</th>\n",
       "      <th>N3_C</th>\n",
       "      <th>Unnamed: 84</th>\n",
       "      <th>IMG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>December</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FedEx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abandon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4502</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4503</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zip</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4504</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zipcode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4505</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zonk_out</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4506</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zoom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4507 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Database   No Word     Lemma   WC   ID KRNS    N Mean R Vision  ... Orth  \\\n",
       "0     VerbNet  NaN  NaN  December  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "1     VerbNet  NaN  NaN     FedEx  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "2     VerbNet  NaN  NaN       UPS  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "3     VerbNet  NaN  NaN   abandon  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "4     VerbNet  NaN  NaN     abase  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "...       ...  ...  ...       ...  ...  ...  ...  ...    ...    ...  ...  ...   \n",
       "4502  VerbNet  NaN  NaN      zing  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "4503  VerbNet  NaN  NaN       zip  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "4504  VerbNet  NaN  NaN   zipcode  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "4505  VerbNet  NaN  NaN  zonk_out  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "4506  VerbNet  NaN  NaN      zoom  NaN  NaN  NaN  NaN    NaN    NaN  ...  NaN   \n",
       "\n",
       "     Orth_F N1_F N1_C N2_F N2_C N3_F N3_C Unnamed: 84  IMG  \n",
       "0       NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "1       NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "2       NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "3       NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "4       NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "...     ...  ...  ...  ...  ...  ...  ...         ...  ...  \n",
       "4502    NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "4503    NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "4504    NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "4505    NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "4506    NaN  NaN  NaN  NaN  NaN  NaN  NaN         NaN  NaN  \n",
       "\n",
       "[4507 rows x 88 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbnet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine CREA df and VerbNet df\n",
    "all_df = crea_verb_df.append(verbnet_df, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "#get the list of thematic roles from VerbNet \n",
    "classids = vn.classids()\n",
    "themroles = []\n",
    "for id in classids: \n",
    "    liszt = vn.themroles(id)\n",
    "    for i in range(len(liszt)):\n",
    "        themroles.append(liszt[i]['type'])\n",
    "\n",
    "print (len(set(themroles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "thematic_roles = (set(themroles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make list of new columns to be added to all_df\n",
    "col_from_verbnet = sorted(list(set(themroles))) + all_semantic_preds\n",
    "len(col_from_verbnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the following checks if column names from VerbNet already exist in crea_verb_df\n",
    "[x for x in crea_verb_df.columns if x in col_from_verbnet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Database', 'No', 'Word', 'Lemma', 'WC', 'ID', 'KRNS', 'N', 'Mean R',\n",
       "       'Vision', 'Bright', 'Dark', 'Color', 'Pattern', 'Large', 'Small',\n",
       "       'Motion', 'Biomotion', 'Fast', 'Slow', 'Shape', 'Complexity', 'Face',\n",
       "       'Body', 'Touch', 'Temperature', 'Texture', 'Weight', 'Pain', 'Audition',\n",
       "       'Loud', 'Low', 'High', 'Sound', 'Music', 'Speech', 'Taste', 'Smell',\n",
       "       'Head', 'UpperLimb', 'LowerLimb', 'Manipulation', 'Object', 'Landmark',\n",
       "       'PathCREA', 'Scene', 'Near', 'Toward', 'Away', 'Number', 'TimeCREA',\n",
       "       'DurationCREA', 'Long', 'Short', 'Caused', 'Consequential', 'Social',\n",
       "       'Human', 'Communication', 'Self', 'Cognition', 'Benefit', 'Harm',\n",
       "       'Pleasant', 'Unpleasant', 'Happy', 'Sad', 'Angry', 'Disgusted',\n",
       "       'Fearful', 'Surprised', 'Drive', 'Needs', 'Attention', 'Arousal',\n",
       "       'STRING', 'LEN', 'FREQ', 'Orth', 'Orth_F', 'N1_F', 'N1_C', 'N2_F',\n",
       "       'N2_C', 'N3_F', 'N3_C', 'Unnamed: 84', 'IMG'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add themroles to add_df as columns\n",
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df.reindex(columns=all_df.columns.to_list() + col_from_verbnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Database', 'No', 'Word', 'Lemma', 'WC', 'ID', 'KRNS', 'N', 'Mean R',\n",
       "       'Vision',\n",
       "       ...\n",
       "       'use', 'utilize', 'value', 'visible', 'void', 'wear', 'weather',\n",
       "       'withdraw', 'work', 'yield'],\n",
       "      dtype='object', length=279)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.set_index('Lemma', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_csv('/Users/songkim/Google Drive/Primary/Projects/VerbVector/verbvector_demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v3.classids('hit')\n",
    "\n",
    "# v3.fileids('bump-18.4')\n",
    "\n",
    "# v3.frames('bump-18.4')\n",
    "\n",
    "# v3.themroles(v3.vnclass(v3.fileids('hit-18.1')[0]))\n",
    "\n",
    "# v3.themroles(v3.vnclass(v3.classids('hit')[0]))\n",
    "\n",
    "# v3.fileids('reach-51.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether there is only one fileid for a given classid\n",
    "all_classid = vn.classids()\n",
    "for a in all_classid:\n",
    "    if len(vn.fileids(a)) != 1:\n",
    "        print (a, 'len of fileid not 1')\n",
    "\n",
    "#nothing prints out, confirming only one fileid for a classid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch themroles for each lemma (going to be removed)\n",
    "# themrole_dict = {}\n",
    "# for l in v3.lemmas():\n",
    "#     c_id = v3.classids(l)\n",
    "#     f_id = v3.fileids(c_id)[0]\n",
    "#     vnclass = v3.vnclass(f_id)\n",
    "#     themrole = v3.themroles(vnclass)\n",
    "#     if l in themrole_dict.keys():\n",
    "#         print (\"lemma alreay exists\")\n",
    "#     else: \n",
    "#         themrole_dict[l] = themrole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# c_id = 'reach-51.8'\n",
    "# f_id = vn.fileids(c_id)[0]\n",
    "# vnclass = vn.vnclass(f_id)\n",
    "# vn.themroles(vnclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-ead2465f5106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassid_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclassid_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v3' is not defined"
     ]
    }
   ],
   "source": [
    "classid_dict = {}\n",
    "for i in v3.lemmas():\n",
    "    c_id = v3.classids(i)\n",
    "    classid_dict[i]=c_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['December',\n",
       " 'FedEx',\n",
       " 'UPS',\n",
       " 'abandon',\n",
       " 'abase',\n",
       " 'abash',\n",
       " 'abate',\n",
       " 'abbreviate',\n",
       " 'abdicate',\n",
       " 'abduct',\n",
       " 'abet',\n",
       " 'abhor',\n",
       " 'abide-by',\n",
       " 'abnegate',\n",
       " 'abolish',\n",
       " 'abound',\n",
       " 'abrade',\n",
       " 'abridge',\n",
       " 'absolve',\n",
       " 'absorb',\n",
       " 'abstain',\n",
       " 'abstract',\n",
       " 'abuse',\n",
       " 'abut',\n",
       " 'accede',\n",
       " 'accelerate',\n",
       " 'accept',\n",
       " 'acclaim',\n",
       " 'accommodate',\n",
       " 'accompany',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'accredit',\n",
       " 'accrue',\n",
       " 'accumulate',\n",
       " 'accuse',\n",
       " 'acetify',\n",
       " 'ache',\n",
       " 'achieve',\n",
       " 'acidify',\n",
       " 'acknowledge',\n",
       " 'acquaint',\n",
       " 'acquiesce',\n",
       " 'acquire',\n",
       " 'acquit',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activate',\n",
       " 'actuate',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'addict',\n",
       " 'address',\n",
       " 'adhere',\n",
       " 'adjoin',\n",
       " 'adjudge',\n",
       " 'adjust',\n",
       " 'administer',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'admix',\n",
       " 'admonish',\n",
       " 'adopt',\n",
       " 'adore',\n",
       " 'adorn',\n",
       " 'advance',\n",
       " 'advert',\n",
       " 'advertise',\n",
       " 'advise',\n",
       " 'affect',\n",
       " 'affiliate',\n",
       " 'affirm',\n",
       " 'affix',\n",
       " 'afflict',\n",
       " 'affront',\n",
       " 'africanize',\n",
       " 'age',\n",
       " 'agglomerate',\n",
       " 'agglutinate',\n",
       " 'aggravate',\n",
       " 'aggregate',\n",
       " 'aggrieve',\n",
       " 'agitate',\n",
       " 'agonize',\n",
       " 'agree',\n",
       " 'aid',\n",
       " 'ail',\n",
       " 'aim',\n",
       " 'air',\n",
       " 'airmail',\n",
       " 'alarm',\n",
       " 'alcoholize',\n",
       " 'alert',\n",
       " 'alienate',\n",
       " 'alkalify',\n",
       " 'alkalize',\n",
       " 'allege',\n",
       " 'alleviate',\n",
       " 'allocate',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allure',\n",
       " 'ally',\n",
       " 'alter',\n",
       " 'alternate',\n",
       " 'amalgamate',\n",
       " 'amass',\n",
       " 'amaze',\n",
       " 'amble',\n",
       " 'ambulate',\n",
       " 'ameliorate',\n",
       " 'amerce',\n",
       " 'americanize',\n",
       " 'amount',\n",
       " 'amplify',\n",
       " 'amuse',\n",
       " 'analyse',\n",
       " 'analyze',\n",
       " 'anchor',\n",
       " 'anesthetize',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'anglicize',\n",
       " 'anguish',\n",
       " 'animate',\n",
       " 'annex',\n",
       " 'annihilate',\n",
       " 'annotate',\n",
       " 'announce',\n",
       " 'annoy',\n",
       " 'annul',\n",
       " 'anoint',\n",
       " 'answer',\n",
       " 'antagonize',\n",
       " 'anticipate',\n",
       " 'antique',\n",
       " 'apostatize',\n",
       " 'appall',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appease',\n",
       " 'applaud',\n",
       " 'applique',\n",
       " 'apply',\n",
       " 'appoint',\n",
       " 'apportion',\n",
       " 'appraise',\n",
       " 'appreciate',\n",
       " 'apprehend',\n",
       " 'apprentice',\n",
       " 'apprise',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'approve',\n",
       " 'approximate',\n",
       " 'arch',\n",
       " 'archive',\n",
       " 'argue',\n",
       " 'arise',\n",
       " 'arm',\n",
       " 'arm-twist',\n",
       " 'armor',\n",
       " 'aromatize',\n",
       " 'arouse',\n",
       " 'arrange',\n",
       " 'arrest',\n",
       " 'arrive',\n",
       " 'arrogate',\n",
       " 'articulate',\n",
       " 'ascend',\n",
       " 'ascertain',\n",
       " 'ask',\n",
       " 'asphalt',\n",
       " 'asphyxiate',\n",
       " 'assail',\n",
       " 'assassinate',\n",
       " 'assault',\n",
       " 'assay',\n",
       " 'assemble',\n",
       " 'assert',\n",
       " 'assess',\n",
       " 'assign',\n",
       " 'assimilate',\n",
       " 'assist',\n",
       " 'associate',\n",
       " 'assuage',\n",
       " 'assume',\n",
       " 'assure',\n",
       " 'astonish',\n",
       " 'astound',\n",
       " 'atomize',\n",
       " 'atrophy',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attain',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attenuate',\n",
       " 'attire',\n",
       " 'attract',\n",
       " 'auction',\n",
       " 'audit',\n",
       " 'augment',\n",
       " 'author',\n",
       " 'authorize',\n",
       " 'autograph',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'avow',\n",
       " 'awake',\n",
       " 'awaken',\n",
       " 'award',\n",
       " 'awe',\n",
       " 'baa',\n",
       " 'babble',\n",
       " 'babysit',\n",
       " 'back',\n",
       " 'back_down',\n",
       " 'back_off',\n",
       " 'back_out',\n",
       " 'backbite',\n",
       " 'backfill',\n",
       " 'backpack',\n",
       " 'badger',\n",
       " 'badmouth',\n",
       " 'baffle',\n",
       " 'bag',\n",
       " 'bail',\n",
       " 'bait',\n",
       " 'bake',\n",
       " 'balance',\n",
       " 'balk',\n",
       " 'ball',\n",
       " 'balloon',\n",
       " 'bamboozle',\n",
       " 'ban',\n",
       " 'band',\n",
       " 'bandage',\n",
       " 'bang',\n",
       " 'bang_away',\n",
       " 'banish',\n",
       " 'bank',\n",
       " 'banquet',\n",
       " 'banter',\n",
       " 'baptize',\n",
       " 'bar',\n",
       " 'barbecue',\n",
       " 'barbeque',\n",
       " 'bare',\n",
       " 'barf',\n",
       " 'bargain',\n",
       " 'barge',\n",
       " 'bark',\n",
       " 'barrack',\n",
       " 'barter',\n",
       " 'base',\n",
       " 'bash',\n",
       " 'bask',\n",
       " 'bastardize',\n",
       " 'baste',\n",
       " 'bat',\n",
       " 'bathe',\n",
       " 'batter',\n",
       " 'battle',\n",
       " 'bawl',\n",
       " 'bay',\n",
       " 'be',\n",
       " 'be_like',\n",
       " 'beach',\n",
       " 'bead',\n",
       " 'beam',\n",
       " 'bear',\n",
       " 'bear_on',\n",
       " 'beard',\n",
       " 'beat',\n",
       " 'beautify',\n",
       " 'beckon',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'bedew',\n",
       " 'beep',\n",
       " 'befall',\n",
       " 'befriend',\n",
       " 'befuddle',\n",
       " 'beg',\n",
       " 'beget',\n",
       " 'beggar',\n",
       " 'begin',\n",
       " 'beguile',\n",
       " 'behave',\n",
       " 'behead',\n",
       " 'behold',\n",
       " 'belch',\n",
       " 'believe',\n",
       " 'belittle',\n",
       " 'bellow',\n",
       " 'bellyache',\n",
       " 'belong',\n",
       " 'belt',\n",
       " 'bench',\n",
       " 'bend',\n",
       " 'benefit',\n",
       " 'bequeath',\n",
       " 'berate',\n",
       " 'bereave',\n",
       " 'berry',\n",
       " 'berth',\n",
       " 'bestrew',\n",
       " 'bestride',\n",
       " 'bet',\n",
       " 'betroth',\n",
       " 'better',\n",
       " 'bewail',\n",
       " 'beware',\n",
       " 'bewilder',\n",
       " 'bewitch',\n",
       " 'bias',\n",
       " 'bicker',\n",
       " 'bicycle',\n",
       " 'biff',\n",
       " 'bifurcate',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'billet',\n",
       " 'billow',\n",
       " 'bin',\n",
       " 'bind',\n",
       " 'birch',\n",
       " 'birdnest',\n",
       " 'birth',\n",
       " 'bisect',\n",
       " 'bitch',\n",
       " 'bite',\n",
       " 'bivouac',\n",
       " 'blab',\n",
       " 'blabber',\n",
       " 'black',\n",
       " 'black_out',\n",
       " 'blackberry',\n",
       " 'blacken',\n",
       " 'blackmail',\n",
       " 'blacktop',\n",
       " 'blame',\n",
       " 'blanch',\n",
       " 'blanket',\n",
       " 'blare',\n",
       " 'blaspheme',\n",
       " 'blast',\n",
       " 'blat',\n",
       " 'blaze',\n",
       " 'bleach',\n",
       " 'bleat',\n",
       " 'bleed',\n",
       " 'blemish',\n",
       " 'blench',\n",
       " 'blend',\n",
       " 'bless',\n",
       " 'blight',\n",
       " 'blindfold',\n",
       " 'blink',\n",
       " 'blister',\n",
       " 'blitz',\n",
       " 'block',\n",
       " 'blockade',\n",
       " 'bloody',\n",
       " 'bloom',\n",
       " 'blossom',\n",
       " 'blot',\n",
       " 'blow',\n",
       " 'blubber',\n",
       " 'bludgeon',\n",
       " 'bluff',\n",
       " 'blunt',\n",
       " 'blur',\n",
       " 'blurt',\n",
       " 'blush',\n",
       " 'bluster',\n",
       " 'board',\n",
       " 'boast',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'bobsled',\n",
       " 'boggle',\n",
       " 'boil',\n",
       " 'boil_down',\n",
       " 'bolshevize',\n",
       " 'bolster',\n",
       " 'bolt',\n",
       " 'bombard',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'bonk',\n",
       " 'boogie',\n",
       " 'book',\n",
       " 'boom',\n",
       " 'boost',\n",
       " 'boot',\n",
       " 'bop',\n",
       " 'border',\n",
       " 'bore',\n",
       " 'borrow',\n",
       " 'boss',\n",
       " 'botch',\n",
       " 'bother',\n",
       " 'bottle',\n",
       " 'bottlefeed',\n",
       " 'bounce',\n",
       " 'bound',\n",
       " 'bow',\n",
       " 'bow_out',\n",
       " 'bowdlerize',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boycott',\n",
       " 'bracket',\n",
       " 'brag',\n",
       " 'braid',\n",
       " 'brain',\n",
       " 'braise',\n",
       " 'branch',\n",
       " 'brand',\n",
       " 'brave',\n",
       " 'brawl',\n",
       " 'bray',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'break_apart',\n",
       " 'break_down',\n",
       " 'break_up',\n",
       " 'breakfast',\n",
       " 'breast',\n",
       " 'breastfeed',\n",
       " 'breathe',\n",
       " 'breeze',\n",
       " 'brew',\n",
       " 'bribe',\n",
       " 'brick',\n",
       " 'bridge',\n",
       " 'bridle',\n",
       " 'brief',\n",
       " 'brighten',\n",
       " 'bring',\n",
       " 'bring-off',\n",
       " 'bring-out',\n",
       " 'bring-up',\n",
       " 'bring_about',\n",
       " 'bring_forth',\n",
       " 'bring_in',\n",
       " 'bristle',\n",
       " 'broadcast',\n",
       " 'broaden',\n",
       " 'broil',\n",
       " 'bronze',\n",
       " 'brood',\n",
       " 'browbeat',\n",
       " 'brown',\n",
       " 'bruise',\n",
       " 'brunch',\n",
       " 'brush',\n",
       " 'bubble',\n",
       " 'buck',\n",
       " 'bucket',\n",
       " 'buckle',\n",
       " 'bud',\n",
       " 'buff',\n",
       " 'buffet',\n",
       " 'bug',\n",
       " 'build',\n",
       " 'build-in',\n",
       " 'build-up',\n",
       " 'bulge',\n",
       " 'bullet',\n",
       " 'bullock',\n",
       " 'bullshit',\n",
       " 'bully',\n",
       " 'bump',\n",
       " 'bundle',\n",
       " 'bungle',\n",
       " 'bunk',\n",
       " 'bunt',\n",
       " 'buoy',\n",
       " 'burble',\n",
       " 'burden',\n",
       " 'burgeon',\n",
       " 'burgle',\n",
       " 'burl',\n",
       " 'burn',\n",
       " 'burp',\n",
       " 'burr',\n",
       " 'burrow',\n",
       " 'burst',\n",
       " 'bury',\n",
       " 'bus',\n",
       " 'bushwhack',\n",
       " 'bust',\n",
       " 'bustle',\n",
       " 'butcher',\n",
       " 'butler',\n",
       " 'butt',\n",
       " 'butter',\n",
       " 'button',\n",
       " 'buttonhole',\n",
       " 'buttress',\n",
       " 'buy',\n",
       " 'buy_the_farm',\n",
       " 'buzz',\n",
       " 'cab',\n",
       " 'cabbage',\n",
       " 'cable',\n",
       " 'cackle',\n",
       " 'caddy',\n",
       " 'cadge',\n",
       " 'cage',\n",
       " 'cajole',\n",
       " 'calcify',\n",
       " 'calibrate',\n",
       " 'calk',\n",
       " 'call',\n",
       " 'call_forth',\n",
       " 'call_on',\n",
       " 'calm',\n",
       " 'calm_down',\n",
       " 'calumniate',\n",
       " 'calve',\n",
       " 'camouflage',\n",
       " 'camp',\n",
       " 'can',\n",
       " 'cancan',\n",
       " 'cane',\n",
       " 'canoe',\n",
       " 'canter',\n",
       " 'canvass',\n",
       " 'cap',\n",
       " 'capacitate',\n",
       " 'caper',\n",
       " 'capitulate',\n",
       " 'capsize',\n",
       " 'captain',\n",
       " 'captivate',\n",
       " 'capture',\n",
       " 'caramelize',\n",
       " 'caravan',\n",
       " 'carbonify',\n",
       " 'carbonize',\n",
       " 'care',\n",
       " 'caress',\n",
       " 'carjack',\n",
       " 'carol',\n",
       " 'carom',\n",
       " 'carpet',\n",
       " 'carry',\n",
       " 'carry_on',\n",
       " 'carry_out',\n",
       " 'carry_through',\n",
       " 'cart',\n",
       " 'carve',\n",
       " 'cascade',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cast',\n",
       " 'castigate',\n",
       " 'castrate',\n",
       " 'catalogue',\n",
       " 'catapult',\n",
       " 'catch',\n",
       " 'categorize',\n",
       " 'caterwaul',\n",
       " 'catholicize',\n",
       " 'catnap',\n",
       " 'caulk',\n",
       " 'cause',\n",
       " 'cauterize',\n",
       " 'caution',\n",
       " 'cavort',\n",
       " 'caw',\n",
       " 'cease',\n",
       " 'cede',\n",
       " 'celebrate',\n",
       " 'cellar',\n",
       " 'cement',\n",
       " 'censure',\n",
       " 'center',\n",
       " 'centralize',\n",
       " 'certify',\n",
       " 'chafe',\n",
       " 'chagrin',\n",
       " 'chain',\n",
       " 'chalk',\n",
       " 'chamfer',\n",
       " 'champion',\n",
       " 'chance',\n",
       " 'chance_across',\n",
       " 'chance_on',\n",
       " 'chance_upon',\n",
       " 'change',\n",
       " 'change_over',\n",
       " 'channel',\n",
       " 'channelize',\n",
       " 'chant',\n",
       " 'chaperone',\n",
       " 'char',\n",
       " 'characterize',\n",
       " 'charbroil',\n",
       " 'charcoal',\n",
       " 'charcoal-broil',\n",
       " 'charge',\n",
       " 'chariot',\n",
       " 'charm',\n",
       " 'chart',\n",
       " 'charter',\n",
       " 'chase',\n",
       " 'chasten',\n",
       " 'chastise',\n",
       " 'chat',\n",
       " 'chatter',\n",
       " 'chauffeur',\n",
       " 'cheapen',\n",
       " 'cheat',\n",
       " 'check',\n",
       " 'cheep',\n",
       " 'cheer',\n",
       " 'cheer_up',\n",
       " 'cherish',\n",
       " 'chew',\n",
       " 'chicken_out',\n",
       " 'chide',\n",
       " 'chill',\n",
       " 'chime',\n",
       " 'chink',\n",
       " 'chip',\n",
       " 'chir',\n",
       " 'chirp',\n",
       " 'chirrup',\n",
       " 'chisel',\n",
       " 'chitchat',\n",
       " 'chitter',\n",
       " 'chlorinate',\n",
       " 'choke',\n",
       " 'chomp',\n",
       " 'choose',\n",
       " 'chop',\n",
       " 'choreograph',\n",
       " 'chortle',\n",
       " 'christen',\n",
       " 'christianize',\n",
       " 'chrome',\n",
       " 'chronicle',\n",
       " 'chuck',\n",
       " 'chuckle',\n",
       " 'chug',\n",
       " 'chunk',\n",
       " 'chunter',\n",
       " 'churn',\n",
       " 'cinch',\n",
       " 'circle',\n",
       " 'circularize',\n",
       " 'circulate',\n",
       " 'circumcise',\n",
       " 'circumscribe',\n",
       " 'circumvent',\n",
       " 'cite',\n",
       " 'civilize',\n",
       " 'clack',\n",
       " 'clad',\n",
       " 'claim',\n",
       " 'clam',\n",
       " 'clamber',\n",
       " 'clamp',\n",
       " 'clang',\n",
       " 'clank',\n",
       " 'clap',\n",
       " 'clarify',\n",
       " 'clash',\n",
       " 'clasp',\n",
       " 'class',\n",
       " 'classify',\n",
       " 'clatter',\n",
       " 'claw',\n",
       " 'clay',\n",
       " 'clean',\n",
       " 'cleanse',\n",
       " 'clear',\n",
       " 'cleave',\n",
       " 'clench',\n",
       " 'clerk',\n",
       " 'click',\n",
       " 'climb',\n",
       " 'cling',\n",
       " 'clink',\n",
       " 'clip',\n",
       " 'cloak',\n",
       " 'clobber',\n",
       " 'clock',\n",
       " 'clog',\n",
       " 'cloister',\n",
       " 'clomp',\n",
       " 'close',\n",
       " 'clothe',\n",
       " 'cloud',\n",
       " 'clout',\n",
       " 'cloy',\n",
       " 'club',\n",
       " 'cluck',\n",
       " 'clump',\n",
       " 'clunk',\n",
       " 'cluster',\n",
       " 'clutch',\n",
       " 'clutter',\n",
       " 'co-occur',\n",
       " 'coach',\n",
       " 'coagulate',\n",
       " 'coal',\n",
       " 'coalesce',\n",
       " 'coarsen',\n",
       " 'coast',\n",
       " 'coat',\n",
       " 'coax',\n",
       " 'cock',\n",
       " 'coddle',\n",
       " 'code',\n",
       " 'coerce',\n",
       " 'coexist',\n",
       " 'cognise',\n",
       " 'cognize',\n",
       " 'cohere',\n",
       " 'coif',\n",
       " 'coil',\n",
       " 'coin',\n",
       " 'coincide',\n",
       " 'coldcream',\n",
       " 'collaborate',\n",
       " 'collapse',\n",
       " 'collar',\n",
       " 'collate',\n",
       " 'collect',\n",
       " 'collide',\n",
       " 'collude',\n",
       " 'color',\n",
       " 'comb',\n",
       " 'combat',\n",
       " 'combine',\n",
       " 'combust',\n",
       " 'come',\n",
       " 'come_across',\n",
       " 'come_around',\n",
       " 'come_off',\n",
       " 'come_out',\n",
       " 'comfort',\n",
       " 'command',\n",
       " 'commandeer',\n",
       " 'commemorate',\n",
       " 'commence',\n",
       " 'commend',\n",
       " 'comment',\n",
       " 'commercialize',\n",
       " 'commingle',\n",
       " 'commiserate',\n",
       " 'commission',\n",
       " 'commit',\n",
       " 'communicate',\n",
       " 'compact',\n",
       " 'compare',\n",
       " 'compel',\n",
       " 'compensate',\n",
       " 'compete',\n",
       " 'compile',\n",
       " 'complain',\n",
       " 'complete',\n",
       " 'compliment',\n",
       " 'compose',\n",
       " 'compound',\n",
       " 'comprehend',\n",
       " 'compress',\n",
       " 'comprise',\n",
       " 'compromise',\n",
       " 'compute',\n",
       " 'con',\n",
       " 'concatenate',\n",
       " 'conceal',\n",
       " 'concede',\n",
       " 'conceive',\n",
       " 'concentrate',\n",
       " 'concern',\n",
       " 'conclude',\n",
       " 'concoct',\n",
       " 'concur',\n",
       " 'condemn',\n",
       " 'condense',\n",
       " 'condition',\n",
       " 'condone',\n",
       " 'conduct',\n",
       " 'confederate',\n",
       " 'confer',\n",
       " 'confess',\n",
       " 'confide',\n",
       " 'configure',\n",
       " 'confine',\n",
       " 'confirm',\n",
       " 'confiscate',\n",
       " 'conflate',\n",
       " 'conform',\n",
       " 'confound',\n",
       " 'confront',\n",
       " 'confuse',\n",
       " 'conga',\n",
       " 'congeal',\n",
       " 'congratulate',\n",
       " 'congregate',\n",
       " 'conjecture',\n",
       " 'conjoin',\n",
       " 'conjure',\n",
       " 'conk',\n",
       " 'connect',\n",
       " 'conquer',\n",
       " 'consecrate',\n",
       " 'consent',\n",
       " 'consider',\n",
       " 'consist_of',\n",
       " 'console',\n",
       " 'consolidate',\n",
       " 'conspire',\n",
       " 'constellate',\n",
       " 'constipate',\n",
       " 'constitute',\n",
       " 'constitutionalize',\n",
       " 'constrain',\n",
       " 'constrict',\n",
       " 'constringe',\n",
       " 'construct',\n",
       " 'construe',\n",
       " 'consult',\n",
       " 'consume',\n",
       " 'contain',\n",
       " 'contaminate',\n",
       " 'contemplate',\n",
       " 'contend',\n",
       " 'content',\n",
       " 'contextualize',\n",
       " 'continue',\n",
       " 'contort',\n",
       " 'contract',\n",
       " 'contrast',\n",
       " 'contribute',\n",
       " 'contrive',\n",
       " 'control',\n",
       " 'contuse',\n",
       " 'convene',\n",
       " 'converge',\n",
       " 'converse',\n",
       " 'convert',\n",
       " 'convey',\n",
       " 'convince',\n",
       " 'convulse',\n",
       " 'coo',\n",
       " 'cook',\n",
       " 'cook_up',\n",
       " 'cool',\n",
       " 'coop',\n",
       " 'cooperate',\n",
       " 'cop',\n",
       " 'cope',\n",
       " 'copy',\n",
       " 'copyright',\n",
       " 'core',\n",
       " 'cork',\n",
       " 'coronate',\n",
       " 'corral',\n",
       " 'correct',\n",
       " 'correlate',\n",
       " 'correspond',\n",
       " 'corroborate',\n",
       " 'corrode',\n",
       " 'corrugate',\n",
       " 'corrupt',\n",
       " 'cosh',\n",
       " 'cost',\n",
       " 'cough',\n",
       " 'counsel',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'counteract',\n",
       " 'couple',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'covet',\n",
       " 'cow',\n",
       " 'cower',\n",
       " 'cowrite',\n",
       " 'cox',\n",
       " 'cozen',\n",
       " 'crab',\n",
       " 'crack',\n",
       " 'crackle',\n",
       " 'craft',\n",
       " 'cram',\n",
       " 'crane',\n",
       " 'crash',\n",
       " 'crate',\n",
       " 'crave',\n",
       " 'crawl',\n",
       " 'crayon',\n",
       " 'creak',\n",
       " 'cream',\n",
       " 'crease',\n",
       " 'create',\n",
       " 'credential',\n",
       " 'credit',\n",
       " 'creep',\n",
       " 'cremate',\n",
       " 'crepitate',\n",
       " 'crest',\n",
       " 'crew',\n",
       " 'crimp',\n",
       " 'crimson',\n",
       " 'cringe',\n",
       " 'crinkle',\n",
       " 'cripple',\n",
       " 'crisp',\n",
       " 'crisscross',\n",
       " 'criticize',\n",
       " 'croak',\n",
       " 'crochet',\n",
       " 'crook',\n",
       " 'croon',\n",
       " 'crop',\n",
       " 'cross',\n",
       " 'cross-examine',\n",
       " 'crouch',\n",
       " 'crow',\n",
       " 'crowd',\n",
       " 'crown',\n",
       " 'crucify',\n",
       " 'cruise',\n",
       " 'crumb',\n",
       " 'crumble',\n",
       " 'crumple',\n",
       " 'crunch',\n",
       " 'crush',\n",
       " 'crust',\n",
       " 'cry',\n",
       " 'crystallize',\n",
       " 'cub',\n",
       " 'cube',\n",
       " 'cuckold',\n",
       " 'cuckoo',\n",
       " 'cuddle',\n",
       " 'cudgel',\n",
       " 'cuff',\n",
       " 'cull',\n",
       " 'cultivate',\n",
       " 'cup',\n",
       " 'curb',\n",
       " 'curdle',\n",
       " 'cure',\n",
       " 'curl',\n",
       " 'curry',\n",
       " 'curse',\n",
       " 'curtain',\n",
       " 'curtsey',\n",
       " 'curve',\n",
       " 'cut',\n",
       " 'cut-off',\n",
       " 'cut_off',\n",
       " 'cut_out',\n",
       " 'cycle',\n",
       " 'dab',\n",
       " 'dally',\n",
       " 'dam',\n",
       " 'damage',\n",
       " 'damn',\n",
       " 'dampen',\n",
       " 'dance',\n",
       " 'dangle',\n",
       " 'dapple',\n",
       " 'dare',\n",
       " 'darken',\n",
       " 'dart',\n",
       " 'dash',\n",
       " 'date',\n",
       " 'daub',\n",
       " 'daunt',\n",
       " 'dawdle',\n",
       " 'dawn',\n",
       " 'daze',\n",
       " 'dazzle',\n",
       " 'de-escalate',\n",
       " 'deaccent',\n",
       " 'deactivate',\n",
       " 'deafen',\n",
       " 'deal',\n",
       " 'debark',\n",
       " 'debase',\n",
       " 'debate',\n",
       " 'debone',\n",
       " 'debowel',\n",
       " 'debug',\n",
       " 'debur',\n",
       " 'deburr',\n",
       " 'decamp',\n",
       " 'decapitate',\n",
       " ...]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to check what the next cell does\n",
    "# c_id = vn.classids('hit')\n",
    "# print ('c_id\\n', c_id)\n",
    "# ln = len(c_id)\n",
    "# for i in range(ln):\n",
    "#     f_id = vn.fileids(c_id[i])[0]\n",
    "#     vnclass = vn.vnclass(f_id)\n",
    "#     troles = vn.themroles(vnclass)\n",
    "#     print ('i is:', i, ', troles\\n', troles)\n",
    "##########IT SEEMS THAT vn.themroles does NOT fetch roles from its subclasses (of vnclass) SHOULD BE FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch themroles for each lemma (updated)\n",
    "themrole_dict = {}\n",
    "for l in vn.lemmas():\n",
    "    c_id = vn.classids(l)\n",
    "    ln = len(c_id)\n",
    "    roles = []\n",
    "    for j in range(ln): \n",
    "        f_id = vn.fileids(c_id[j])[0]\n",
    "        vnclass = vn.vnclass(f_id)\n",
    "        themrole = vn.themroles(vnclass)\n",
    "        roles += themrole\n",
    "    if l in themrole_dict.keys():\n",
    "        print (\"lemma already exists\")\n",
    "    else: \n",
    "        themrole_dict[l] = roles        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body_motion-49.2-1-1',\n",
       " 'get-13.5.1',\n",
       " 'initiate_communication-37.4.2',\n",
       " 'reach-51.8']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn.classids('reach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Agent', 'modifiers': []},\n",
       " {'type': 'Theme', 'modifiers': [{'value': '+', 'type': 'body_part'}]},\n",
       " {'type': 'Goal', 'modifiers': []},\n",
       " {'type': 'Path', 'modifiers': []},\n",
       " {'type': 'Agent',\n",
       "  'modifiers': [{'value': '+', 'type': 'animate'},\n",
       "   {'value': '+', 'type': 'organization'}]},\n",
       " {'type': 'Theme', 'modifiers': []},\n",
       " {'type': 'Source', 'modifiers': [{'value': '+', 'type': 'concrete'}]},\n",
       " {'type': 'Beneficiary',\n",
       "  'modifiers': [{'value': '+', 'type': 'animate'},\n",
       "   {'value': '+', 'type': 'organization'}]},\n",
       " {'type': 'Asset',\n",
       "  'modifiers': [{'value': '-', 'type': 'location'},\n",
       "   {'value': '-', 'type': 'region'}]},\n",
       " {'type': 'Agent',\n",
       "  'modifiers': [{'value': '+', 'type': 'animate'},\n",
       "   {'value': '+', 'type': 'organization'}]},\n",
       " {'type': 'Recipient',\n",
       "  'modifiers': [{'value': '+', 'type': 'animate'},\n",
       "   {'value': '+', 'type': 'organization'}]},\n",
       " {'type': 'Theme',\n",
       "  'modifiers': [{'value': '+', 'type': 'concrete'},\n",
       "   {'value': '+', 'type': 'organization'}]},\n",
       " {'type': 'Goal', 'modifiers': []}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themrole_dict['reach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 44, 50}\n"
     ]
    }
   ],
   "source": [
    "#code to calculate how many themroles the lemmas have \n",
    "length_range = []\n",
    "for value in themrole_dict.values():\n",
    "    l= len(value)\n",
    "    length_range.append(l)\n",
    "print (set(length_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baste', 'walk_through', 'entertain']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_words = random.choices(vn_lemma, k=3)\n",
    "\n",
    "random_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baste [{'type': 'Agent', 'modifiers': [{'value': '+', 'type': 'animate'}, {'value': '+', 'type': 'machine'}]}, {'type': 'Patient', 'modifiers': [{'value': '+', 'type': 'concrete'}]}, {'type': 'Co-Patient', 'modifiers': [{'value': '+', 'type': 'concrete'}]}, {'type': 'Agent', 'modifiers': [{'value': '+', 'type': 'animate'}]}, {'type': 'Theme', 'modifiers': []}, {'type': 'Initial_Location', 'modifiers': [{'value': '+', 'type': 'location'}]}, {'type': 'Destination', 'modifiers': [{'value': '+', 'type': 'location'}, {'value': '-', 'type': 'region'}]}] \n",
      "\n",
      "walk_through [{'type': 'Agent', 'modifiers': [{'value': '+', 'type': 'animate'}, {'value': '+', 'type': 'organization'}]}, {'type': 'Theme', 'modifiers': []}] \n",
      "\n",
      "entertain [{'type': 'Experiencer', 'modifiers': [{'value': '+', 'type': 'animate'}]}, {'type': 'Stimulus', 'modifiers': []}, {'type': 'Result', 'modifiers': []}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in random_words:\n",
    "    print (w, themrole_dict[w], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all_df to dictionary\n",
    "all_dict = all_df.to_dict(orient='index')\n",
    "\n",
    "#fill in zero for VERBNET features\n",
    "for key, value in all_dict.items():\n",
    "    for feature in col_from_verbnet: \n",
    "        value[feature] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_zero_filled = pd.DataFrame.from_dict(all_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>No</th>\n",
       "      <th>Word</th>\n",
       "      <th>WC</th>\n",
       "      <th>ID</th>\n",
       "      <th>KRNS</th>\n",
       "      <th>N</th>\n",
       "      <th>Mean R</th>\n",
       "      <th>Vision</th>\n",
       "      <th>Bright</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>utilize</th>\n",
       "      <th>value</th>\n",
       "      <th>visible</th>\n",
       "      <th>void</th>\n",
       "      <th>wear</th>\n",
       "      <th>weather</th>\n",
       "      <th>withdraw</th>\n",
       "      <th>work</th>\n",
       "      <th>yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>approach</td>\n",
       "      <td>CREA</td>\n",
       "      <td>181.0</td>\n",
       "      <td>approached</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.7827</td>\n",
       "      <td>3.0741</td>\n",
       "      <td>0.2593</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>arrest</td>\n",
       "      <td>CREA</td>\n",
       "      <td>182.0</td>\n",
       "      <td>arrested</td>\n",
       "      <td>2.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>1.9667</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>eat</td>\n",
       "      <td>CREA</td>\n",
       "      <td>183.0</td>\n",
       "      <td>ate</td>\n",
       "      <td>2.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.7808</td>\n",
       "      <td>2.5862</td>\n",
       "      <td>0.2759</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>block</td>\n",
       "      <td>CREA</td>\n",
       "      <td>184.0</td>\n",
       "      <td>blocked</td>\n",
       "      <td>2.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.6496</td>\n",
       "      <td>3.2963</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>buy</td>\n",
       "      <td>CREA</td>\n",
       "      <td>185.0</td>\n",
       "      <td>bought</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.7225</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Database     No        Word   WC     ID KRNS     N  Mean R  Vision  \\\n",
       "approach     CREA  181.0  approached  2.0   19.0    Y  27.0  0.7827  3.0741   \n",
       "arrest       CREA  182.0    arrested  2.0  201.0    Y  30.0  0.7622  1.9667   \n",
       "eat          CREA  183.0         ate  2.0  245.0    Y  29.0  0.7808  2.5862   \n",
       "block        CREA  184.0     blocked  2.0  187.0    Y  27.0  0.6496  3.2963   \n",
       "buy          CREA  185.0      bought  2.0   23.0    Y  29.0  0.7225  3.0000   \n",
       "\n",
       "          Bright  ...  use  utilize  value  visible  void  wear  weather  \\\n",
       "approach  0.2593  ...    0        0      0        0     0     0        0   \n",
       "arrest    0.3000  ...    0        0      0        0     0     0        0   \n",
       "eat       0.2759  ...    0        0      0        0     0     0        0   \n",
       "block     0.4444  ...    0        0      0        0     0     0        0   \n",
       "buy       0.2414  ...    0        0      0        0     0     0        0   \n",
       "\n",
       "          withdraw  work  yield  \n",
       "approach         0     0      0  \n",
       "arrest           0     0      0  \n",
       "eat              0     0      0  \n",
       "block            0     0      0  \n",
       "buy              0     0      0  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df_zero_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_zero_filled.to_csv('/Users/songkim/Google Drive/Primary/Projects/VerbVector/verbvector_interim.csv')\n",
    "#all_df_filled should be removed ultimately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in random_words: \n",
    "#     print (len(themrole_dict[w]), themrole_dict[w], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Location', 'modifiers': [{'value': '+', 'type': 'concrete'}]},\n",
       " {'type': 'Theme', 'modifiers': [{'value': '+', 'type': 'concrete'}]},\n",
       " {'type': 'Theme', 'modifiers': [{'value': '+', 'type': 'concrete'}]},\n",
       " {'type': 'Co-Theme', 'modifiers': [{'value': '+', 'type': 'concrete'}]},\n",
       " {'type': 'Agent', 'modifiers': [{'value': '+', 'type': 'int_control'}]},\n",
       " {'type': 'Patient', 'modifiers': [{'value': '+', 'type': 'concrete'}]},\n",
       " {'type': 'Instrument', 'modifiers': [{'value': '+', 'type': 'concrete'}]},\n",
       " {'type': 'Result', 'modifiers': []},\n",
       " {'type': 'Experiencer', 'modifiers': [{'value': '+', 'type': 'animate'}]},\n",
       " {'type': 'Theme',\n",
       "  'modifiers': [{'value': '+', 'type': 'concrete'},\n",
       "   {'value': '+', 'type': 'organization'}]},\n",
       " {'type': 'Goal', 'modifiers': []},\n",
       " {'type': 'Agent', 'modifiers': [{'value': '+', 'type': 'int_control'}]},\n",
       " {'type': 'Theme', 'modifiers': [{'value': '+', 'type': 'concrete'}]},\n",
       " {'type': 'Initial_Location',\n",
       "  'modifiers': [{'value': '+', 'type': 'location'}]},\n",
       " {'type': 'Destination', 'modifiers': [{'value': '+', 'type': 'animate'}]},\n",
       " {'type': 'Result', 'modifiers': []}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print('value =', themrole_dict['December']\n",
    "themrole_dict['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create another dictionary mapping a lemma to its themroles. ex {'inveigle: ['Agent', 'Patient', Predicate]'}\n",
    "themrole_dict_compact = {}\n",
    "for key, value in themrole_dict.items(): \n",
    "    n = len(value)\n",
    "    themrole_list = []\n",
    "    for i in range(n):\n",
    "        themrole_list.append(value[i]['type'])\n",
    "    themrole_dict_compact[key]=set(themrole_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#themrole_dict_compact['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_dict['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of Semantic Predicates used for each lemma\n",
    "def semantic_pred_from_lemma(self, lemma):\n",
    "    lemma_classids = self.classids(lemma)\n",
    "    pred_list_combined = []\n",
    "    for k in range(len(lemma_classids)):\n",
    "        vnclass = self.vnclass(lemma_classids[k])\n",
    "        frame_list = vnclass.findall('FRAMES/FRAME')\n",
    "        pred_list = []\n",
    "        for i in range(len(frame_list)):\n",
    "            semantics_per_frame = self._get_semantics_within_frame(frame_list[i])\n",
    "            for j in range(len(semantics_per_frame)):\n",
    "                pred = semantics_per_frame[j]['predicate_value']\n",
    "                pred_list.append(pred)\n",
    "        pred_list_combined.append(pred_list)\n",
    "    \n",
    "    pred_list_combined_final = list(set(list(flatten(pred_list_combined))))\n",
    "\n",
    "\n",
    "    return pred_list_combined_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic_pred_from_lemma(vn, 'hit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dict mapping a lemma to its semantic predicates, ex {'hit': ['motion','contact','harmed', 'cause', ...]}\n",
    "semantic_pred_dict = {}\n",
    "for lemma in vn.lemmas():\n",
    "    if lemma in semantic_pred_dict.keys():\n",
    "        print (lemma, \" already exists!\")\n",
    "    else: \n",
    "        semantic_pred_dict[lemma] = semantic_pred_from_lemma(vn, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic_pred_dict\n",
    "#all_dict['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update all_dict so that thematic role columns to be updated (0-->1) where appropriate\n",
    "for k, v in all_dict.items():\n",
    "    themroles = themrole_dict_compact[k]\n",
    "    for t in themroles:\n",
    "        v[t] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update all dict so that Semantic Predicate columns to be updated (0-->1)where appropriate\n",
    "for q, w in all_dict.items():\n",
    "    semantic_preds = semantic_pred_dict[q]\n",
    "    for s in semantic_preds: \n",
    "        w[s] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Database': 'CREA',\n",
       " 'No': 183,\n",
       " 'Word': 'ate',\n",
       " 'WC': 2,\n",
       " 'ID': 245,\n",
       " 'KRNS': 'Y',\n",
       " 'N': 29,\n",
       " 'Mean R': 0.7808,\n",
       " 'Vision': 2.5862,\n",
       " 'Bright': 0.2759,\n",
       " 'Dark': 0.069,\n",
       " 'Color': 0.3793,\n",
       " 'Pattern': 0.6207,\n",
       " 'Large': 0.8621,\n",
       " 'Small': 2.3448,\n",
       " 'Motion': 1.931,\n",
       " 'Biomotion': 4.6552,\n",
       " 'Fast': 1.4483,\n",
       " 'Slow': 1.8276,\n",
       " 'Shape': 1.8276,\n",
       " 'Complexity': 'na',\n",
       " 'Face': 5.0345,\n",
       " 'Body': 2.9655,\n",
       " 'Touch': 3.1379,\n",
       " 'Temperature': 2.6897,\n",
       " 'Texture': 1.8621,\n",
       " 'Weight': 1.7241,\n",
       " 'Pain': 0.3103,\n",
       " 'Audition': 0.7241,\n",
       " 'Loud': 0.4483,\n",
       " 'Low': 0.1379,\n",
       " 'High': 0.0,\n",
       " 'Sound': 1.5172,\n",
       " 'Music': 0.0,\n",
       " 'Speech': 0.6207,\n",
       " 'Taste': 5.5172,\n",
       " 'Smell': 3.4828,\n",
       " 'Head': 5.6207,\n",
       " 'UpperLimb': 4.3448,\n",
       " 'LowerLimb': 0.0345,\n",
       " 'Manipulation': '5.4483',\n",
       " 'Object': '4.4483',\n",
       " 'Landmark': 0.3793,\n",
       " 'PathCREA': 0.8276,\n",
       " 'Scene': 3.6897,\n",
       " 'Near': 3.4828,\n",
       " 'Toward': 3.2414,\n",
       " 'Away': 0.5862,\n",
       " 'Number': 0.6897,\n",
       " 'TimeCREA': 3.2759,\n",
       " 'DurationCREA': 2.5862,\n",
       " 'Long': 0.7241,\n",
       " 'Short': 2.7241,\n",
       " 'Caused': '2.4483',\n",
       " 'Consequential': 2.4483,\n",
       " 'Social': 2.0,\n",
       " 'Human': 2.7931,\n",
       " 'Communication': 0.6552,\n",
       " 'Self': 5.1034,\n",
       " 'Cognition': 1.0345,\n",
       " 'Benefit': 3.8966,\n",
       " 'Harm': 0.5517,\n",
       " 'Pleasant': 4.6207,\n",
       " 'Unpleasant': 0.2759,\n",
       " 'Happy': 4.069,\n",
       " 'Sad': 0.4138,\n",
       " 'Angry': 0.1379,\n",
       " 'Disgusted': 0.4483,\n",
       " 'Fearful': 0.1034,\n",
       " 'Surprised': 0.2069,\n",
       " 'Drive': 2.2069,\n",
       " 'Needs': 5.4828,\n",
       " 'Attention': 1.8966,\n",
       " 'Arousal': 2.4828,\n",
       " 'STRING': 'ate',\n",
       " 'LEN': 3,\n",
       " 'FREQ': 10.5608,\n",
       " 'Orth': 9,\n",
       " 'Orth_F': 504.0,\n",
       " 'N1_F': 38891.36,\n",
       " 'N1_C': 38.0,\n",
       " 'N2_F': 42.83,\n",
       " 'N2_C': 1.0,\n",
       " 'N3_F': 42.83,\n",
       " 'N3_C': 1.0,\n",
       " 'Unnamed: 84': nan,\n",
       " 'IMG': 3.7067,\n",
       " 'Affector': 0,\n",
       " 'Agent': 1,\n",
       " 'Asset': 0,\n",
       " 'Attribute': 0,\n",
       " 'Beneficiary': 0,\n",
       " 'Causer': 0,\n",
       " 'Co-Agent': 0,\n",
       " 'Co-Patient': 0,\n",
       " 'Co-Theme': 0,\n",
       " 'Context': 0,\n",
       " 'Destination': 0,\n",
       " 'Duration': 0,\n",
       " 'Experiencer': 0,\n",
       " 'Extent': 0,\n",
       " 'Final_Time': 0,\n",
       " 'Goal': 0,\n",
       " 'Initial_Location': 0,\n",
       " 'Initial_State': 0,\n",
       " 'Instrument': 0,\n",
       " 'Location': 1,\n",
       " 'Manner': 0,\n",
       " 'Material': 0,\n",
       " 'Path': 0,\n",
       " 'Patient': 0,\n",
       " 'Pivot': 0,\n",
       " 'Precondition': 0,\n",
       " 'Predicate': 0,\n",
       " 'Product': 0,\n",
       " 'Recipient': 0,\n",
       " 'Reflexive': 0,\n",
       " 'Result': 0,\n",
       " 'Source': 0,\n",
       " 'Stimulus': 0,\n",
       " 'Theme': 0,\n",
       " 'Time': 0,\n",
       " 'Topic': 0,\n",
       " 'Trajectory': 0,\n",
       " 'Value': 0,\n",
       " 'Adv': 0,\n",
       " 'about': 0,\n",
       " 'act': 0,\n",
       " 'adjust': 0,\n",
       " 'admit': 0,\n",
       " 'adopt': 0,\n",
       " 'agree': 0,\n",
       " 'alive': 0,\n",
       " 'allow': 0,\n",
       " 'apart': 0,\n",
       " 'appear': 0,\n",
       " 'apply_heat': 0,\n",
       " 'apply_material': 0,\n",
       " 'approve': 0,\n",
       " 'assess': 0,\n",
       " 'attached': 0,\n",
       " 'attempt': 0,\n",
       " 'attract': 0,\n",
       " 'authority_relationship': 0,\n",
       " 'avoid': 0,\n",
       " 'base': 0,\n",
       " 'begin': 0,\n",
       " 'believe': 0,\n",
       " 'benefit': 0,\n",
       " 'body_motion': 0,\n",
       " 'body_process': 0,\n",
       " 'body_reflex': 0,\n",
       " 'calculate': 0,\n",
       " 'capacity': 0,\n",
       " 'cause': 0,\n",
       " 'change_value': 0,\n",
       " 'characterize': 0,\n",
       " 'charge': 0,\n",
       " 'conclude': 0,\n",
       " 'confined': 0,\n",
       " 'conflict': 0,\n",
       " 'confront': 0,\n",
       " 'consider': 0,\n",
       " 'conspire': 0,\n",
       " 'contact': 0,\n",
       " 'contain': 0,\n",
       " 'continue': 0,\n",
       " 'convert': 0,\n",
       " 'cooked': 0,\n",
       " 'cooperate': 0,\n",
       " 'cope': 0,\n",
       " 'correlate': 0,\n",
       " 'cost': 0,\n",
       " 'covered': 0,\n",
       " 'created_image': 0,\n",
       " 'declare': 0,\n",
       " 'dedicate': 0,\n",
       " 'defend': 0,\n",
       " 'degradation_material_integrity': 0,\n",
       " 'delay': 0,\n",
       " 'depend': 0,\n",
       " 'describe': 0,\n",
       " 'designated': 0,\n",
       " 'desire': 0,\n",
       " 'destroyed': 0,\n",
       " 'different': 0,\n",
       " 'direction': 0,\n",
       " 'disappear': 0,\n",
       " 'discomfort': 0,\n",
       " 'discover': 0,\n",
       " 'do': 0,\n",
       " 'earn': 0,\n",
       " 'emit': 0,\n",
       " 'emotional_state': 0,\n",
       " 'end': 0,\n",
       " 'enforce': 0,\n",
       " 'ensure': 0,\n",
       " 'equals': 0,\n",
       " 'exceed': 0,\n",
       " 'exert_force': 0,\n",
       " 'exist': 0,\n",
       " 'experience': 0,\n",
       " 'express': 0,\n",
       " 'filled_with': 0,\n",
       " 'financial_interest_in': 0,\n",
       " 'financial_relationship': 0,\n",
       " 'flinch': 0,\n",
       " 'free': 0,\n",
       " 'function': 0,\n",
       " 'give_birth': 0,\n",
       " 'group': 0,\n",
       " 'harmed': 0,\n",
       " 'harmonize': 0,\n",
       " 'has_possession': 0,\n",
       " 'help': 0,\n",
       " 'in_reaction_to': 0,\n",
       " 'indicate': 0,\n",
       " 'involuntary': 0,\n",
       " 'involve': 0,\n",
       " 'license': 0,\n",
       " 'limit': 0,\n",
       " 'linger': 0,\n",
       " 'location': 1,\n",
       " 'made_of': 0,\n",
       " 'manner': 0,\n",
       " 'masquerade': 0,\n",
       " 'meets': 0,\n",
       " 'mingled': 0,\n",
       " 'motion': 0,\n",
       " 'necessitate': 0,\n",
       " 'neglect': 0,\n",
       " 'nonagentive_cause': 0,\n",
       " 'occur': 0,\n",
       " 'path_rel': 0,\n",
       " 'perceive': 0,\n",
       " 'perform': 0,\n",
       " 'physical_form': 0,\n",
       " 'position': 0,\n",
       " 'promote': 0,\n",
       " 'property': 0,\n",
       " 'relate': 0,\n",
       " 'require': 0,\n",
       " 'risk': 0,\n",
       " 'rotational_motion': 0,\n",
       " 'rush': 0,\n",
       " 'satisfy': 0,\n",
       " 'search': 0,\n",
       " 'seem': 0,\n",
       " 'set_member': 0,\n",
       " 'signify': 0,\n",
       " 'sleep': 0,\n",
       " 'social_interaction': 0,\n",
       " 'spend': 0,\n",
       " 'state': 0,\n",
       " 'subjugated': 0,\n",
       " 'successful_in': 0,\n",
       " 'suffocate': 0,\n",
       " 'support': 0,\n",
       " 'suspect': 0,\n",
       " 'take_care_of': 0,\n",
       " 'take_in': 1,\n",
       " 'think': 0,\n",
       " 'time': 0,\n",
       " 'together': 0,\n",
       " 'transfer': 0,\n",
       " 'transfer_info': 0,\n",
       " 'understand': 0,\n",
       " 'urge': 0,\n",
       " 'use': 0,\n",
       " 'utilize': 0,\n",
       " 'value': 0,\n",
       " 'visible': 0,\n",
       " 'void': 0,\n",
       " 'wear': 0,\n",
       " 'weather': 0,\n",
       " 'withdraw': 0,\n",
       " 'work': 0,\n",
       " 'yield': 0}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dict['eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_final = pd.DataFrame.from_dict(all_dict, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_final.to_csv('/Users/songkim/Google Drive/Primary/Projects/VerbVector/verbvec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether cell values for thematic roles are accurate\n",
    "themrole_from_csv = {}\n",
    "for key, val in all_dict.items():\n",
    "    thlist_for_each_key = []\n",
    "    for key2, val2 in val.items():\n",
    "        if val2 == 1 and key2 in thematic_roles:\n",
    "            thlist_for_each_key.append(key2)\n",
    "    themrole_from_csv[key] = thlist_for_each_key\n",
    "    \n",
    "#all_dict['hit'] = {'Database':'CREA', 'Agent':1, 'utilize':nan, 'cause':1...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Agent',\n",
       " 'Co-Theme',\n",
       " 'Destination',\n",
       " 'Experiencer',\n",
       " 'Goal',\n",
       " 'Initial_Location',\n",
       " 'Instrument',\n",
       " 'Location',\n",
       " 'Patient',\n",
       " 'Result',\n",
       " 'Theme']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chec\n",
    "themrole_from_csv['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(themrole_dict_compact['hit'])) == themrole_from_csv['hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in vn.lemmas():\n",
    "    if themrole_from_csv[w] != sorted(list(themrole_dict_compact[w])):\n",
    "        print (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
